{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"de5fe140-f950-4f55-a69d-32b200ad8a7a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/usr/local/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\\n\",\n",
    "      \"  from .autonotebook import tqdm as notebook_tqdm\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\n\",\n",
    "      \"===================================BUG REPORT===================================\\n\",\n",
    "      \"Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\\n\",\n",
    "      \"================================================================================\\n\",\n",
    "      \"CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\\n\",\n",
    "      \"CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\\n\",\n",
    "      \"CUDA SETUP: Highest compute capability among GPUs detected: 8.0\\n\",\n",
    "      \"CUDA SETUP: Detected CUDA version 117\\n\",\n",
    "      \"CUDA SETUP: Loading binary /usr/local/conda/lib/python3.10/site-packages/bitsandbytes-0.37.2-py3.10.egg/bitsandbytes/libbitsandbytes_cuda117.so...\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/usr/local/conda/lib/python3.10/site-packages/bitsandbytes-0.37.2-py3.10.egg/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/extras/CUPTI/lib64'), PosixPath('/usr/local/nvidia/lib64')}\\n\",\n",
    "      \"  warn(msg)\\n\",\n",
    "      \"/usr/local/conda/lib/python3.10/site-packages/bitsandbytes-0.37.2-py3.10.egg/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/extras/CUPTI/lib64 did not contain libcudart.so as expected! Searching further paths...\\n\",\n",
    "      \"  warn(msg)\\n\",\n",
    "      \"/usr/local/conda/lib/python3.10/site-packages/bitsandbytes-0.37.2-py3.10.egg/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\\n\",\n",
    "      \"  warn(msg)\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import bitsandbytes as bnb\\n\",\n",
    "    \"from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\\n\",\n",
    "    \"\\n\",\n",
    "    \"device=\\\"cuda\\\"\\n\",\n",
    "    \"model = AutoModelForCausalLM.from_pretrained(\\\"/workspace/model/bloomz-3b\\\", load_in_8bit=True)\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(\\\"/workspace/model/bloomz-3b\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"id\": \"836e6354-38e2-471c-966e-71614d35c5dc\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/usr/local/conda/lib/python3.10/site-packages/peft/utils/other.py:136: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\\n\",\n",
    "      \"  warnings.warn(\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"from peft import prepare_model_for_int8_training\\n\",\n",
    "    \"model = prepare_model_for_int8_training(model)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"id\": \"45138378-5788-4cbd-a938-d2d969f56c26\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"trainable params: 4915200 || all params: 3007472640 || trainable%: 0.1634329082375293\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"def print_trainable_parameters(model):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Prints the number of trainable parameters in the model.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    trainable_params = 0\\n\",\n",
    "    \"    all_param = 0\\n\",\n",
    "    \"    for _, param in model.named_parameters():\\n\",\n",
    "    \"        all_param += param.numel()\\n\",\n",
    "    \"        if param.requires_grad:\\n\",\n",
    "    \"            trainable_params += param.numel()\\n\",\n",
    "    \"    print(\\n\",\n",
    "    \"        f\\\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\\\"\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"from peft import LoraConfig, get_peft_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"config = LoraConfig(\\n\",\n",
    "    \"    r=16, lora_alpha=32, target_modules=[\\\"query_key_value\\\"], lora_dropout=0.05, bias=\\\"none\\\", task_type=\\\"CAUSAL_LM\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = get_peft_model(model, config)\\n\",\n",
    "    \"model = model.to(device)\\n\",\n",
    "    \"print_trainable_parameters(model)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"id\": \"b482dbfd-95dd-47a6-8762-124d613756dc\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import transformers\\n\",\n",
    "    \"from datasets import load_dataset\\n\",\n",
    "    \"\\n\",\n",
    "    \"# data = load_dataset(\\\"Abirate/english_quotes\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"data = load_dataset(\\\"/workspace/data/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\\\")\\n\",\n",
    "    \"data = data.map(lambda samples: tokenizer(samples[\\\"quote\\\"]), batched=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"id\": \"0de2b961-d394-422c-8ef3-3c55893dec2a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\\n\",\n",
    "      \"/usr/local/conda/lib/python3.10/site-packages/bitsandbytes-0.37.2-py3.10.egg/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\\n\",\n",
    "      \"  warnings.warn(f\\\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\\\")\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"\\n\",\n",
    "       \"    <div>\\n\",\n",
    "       \"      \\n\",\n",
    "       \"      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\\n\",\n",
    "       \"      [20/20 01:55, Epoch 0/1]\\n\",\n",
    "       \"    </div>\\n\",\n",
    "       \"    <table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \" <tr style=\\\"text-align: left;\\\">\\n\",\n",
    "       \"      <th>Step</th>\\n\",\n",
    "       \"      <th>Training Loss</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>3.211400</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>3.373700</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>3.170300</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>3.282100</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>3.187100</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>3.196800</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>2.952900</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>3.212800</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>3.168700</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>3.116700</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>3.131100</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>2.964200</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>2.955700</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>3.010200</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>3.048800</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>3.031200</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>2.885200</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>3.000700</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>2.994000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>20</td>\\n\",\n",
    "       \"      <td>3.083600</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table><p>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"checkpoint folder:  outputs/checkpoint-10\\n\",\n",
    "      \"checkpoint folder list:  ['README.md', 'adapter_config.json', 'adapter_model', 'adapter_model.safetensors', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'trainer_state.json', 'training_args.bin']\\n\",\n",
    "      \"checkpoint adapter folder list:  ['README.md', 'adapter_config.json', 'adapter_model.bin']\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/usr/local/conda/lib/python3.10/site-packages/bitsandbytes-0.37.2-py3.10.egg/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\\n\",\n",
    "      \"  warnings.warn(f\\\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\\\")\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"checkpoint folder:  outputs/checkpoint-20\\n\",\n",
    "      \"checkpoint folder list:  ['README.md', 'adapter_config.json', 'adapter_model', 'adapter_model.safetensors', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'trainer_state.json', 'training_args.bin']\\n\",\n",
    "      \"checkpoint adapter folder list:  ['README.md', 'adapter_config.json', 'adapter_model.bin']\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"TrainOutput(global_step=20, training_loss=3.098853278160095, metrics={'train_runtime': 122.735, 'train_samples_per_second': 10.429, 'train_steps_per_second': 0.163, 'total_flos': 2954973215784960.0, 'train_loss': 3.098853278160095, 'epoch': 0.51})\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 5,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\\n\",\n",
    "    \"from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\\n\",\n",
    "    \"\\n\",\n",
    "    \"class SavePeftModelCallback(TrainerCallback):\\n\",\n",
    "    \"    def on_save(\\n\",\n",
    "    \"        self,\\n\",\n",
    "    \"        args: TrainingArguments,\\n\",\n",
    "    \"        state: TrainerState,\\n\",\n",
    "    \"        control: TrainerControl,\\n\",\n",
    "    \"        **kwargs,\\n\",\n",
    "    \"    ):\\n\",\n",
    "    \"        checkpoint_folder = os.path.join(args.output_dir, f\\\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\\\")\\n\",\n",
    "    \"        print(\\\"checkpoint folder: \\\",checkpoint_folder)\\n\",\n",
    "    \"        peft_model_path = os.path.join(checkpoint_folder, \\\"adapter_model\\\")\\n\",\n",
    "    \"        kwargs[\\\"model\\\"].save_pretrained(peft_model_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        files = os.listdir(checkpoint_folder)\\n\",\n",
    "    \"        print(\\\"checkpoint folder list: \\\", files)\\n\",\n",
    "    \"        adapter_files = os.listdir(peft_model_path)\\n\",\n",
    "    \"        print(\\\"checkpoint adapter folder list: \\\", adapter_files)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        pytorch_model_path = os.path.join(checkpoint_folder, \\\"pytorch_model.bin\\\")\\n\",\n",
    "    \"        if os.path.exists(pytorch_model_path):\\n\",\n",
    "    \"            os.remove(pytorch_model_path)\\n\",\n",
    "    \"        return control\\n\",\n",
    "    \"\\n\",\n",
    "    \"args = transformers.TrainingArguments(\\n\",\n",
    "    \"        per_device_train_batch_size=2,\\n\",\n",
    "    \"        gradient_accumulation_steps=4,\\n\",\n",
    "    \"        warmup_steps=5,\\n\",\n",
    "    \"        max_steps=20,\\n\",\n",
    "    \"        learning_rate=2e-4,\\n\",\n",
    "    \"        fp16=True,\\n\",\n",
    "    \"        logging_steps=1,\\n\",\n",
    "    \"        output_dir=\\\"outputs\\\",\\n\",\n",
    "    \"        save_strategy = 'steps',\\n\",\n",
    "    \"        save_steps = 10\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"trainer = transformers.Trainer(\\n\",\n",
    "    \"    model=model,\\n\",\n",
    "    \"    train_dataset=data[\\\"train\\\"],\\n\",\n",
    "    \"    args=args,\\n\",\n",
    "    \"    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\\n\",\n",
    "    \"    callbacks=[SavePeftModelCallback()],\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\\n\",\n",
    "    \"\\n\",\n",
    "    \"trainer.train()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"id\": \"44cf46c9-dbaf-45f0-9a05-694ce53b74f1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/usr/local/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"/usr/local/conda/lib/python3.10/site-packages/bitsandbytes-0.37.2-py3.10.egg/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\\n\",\n",
    "      \"  warnings.warn(f\\\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\\\")\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"output：\\n\",\n",
    "      \"\\n\",\n",
    "      \" Two things are infinite:  the universe and the number of ways you can screw up.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"from peft import PeftModel, PeftConfig\\n\",\n",
    "    \"from transformers import AutoModelForCausalLM, AutoTokenizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"peft_model_id = \\\"outputs/checkpoint-20/\\\"\\n\",\n",
    "    \"config = PeftConfig.from_pretrained(peft_model_id)\\n\",\n",
    "    \"model = AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "    \"    config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map=\\\"auto\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the Lora model\\n\",\n",
    "    \"model = PeftModel.from_pretrained(model, peft_model_id)\\n\",\n",
    "    \"\\n\",\n",
    "    \"batch = tokenizer(\\\"Two things are infinite: \\\", return_tensors=\\\"pt\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"with torch.cuda.amp.autocast():\\n\",\n",
    "    \"    output_tokens = model.generate(**batch, max_new_tokens=50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"output：\\\\n\\\\n\\\", tokenizer.decode(output_tokens[0], skip_special_tokens=True))\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.9\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
