

- https://github.com/pytorch/examples
- https://github.com/pytorch/examples.git
- https://github.com/pytorch/pytorch

---



- torch.distributed.get_rank() # 取得当前进程的全局序号
- torch.distributed.get_world_size() # 取得全局进程的个数
- torch.cuda.set_device(device) # 为当前进程分配GPU
- torch.distributed.new_group(ranks) # 设置组
- torch.cuda.current_device()




---


- https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html
- 
