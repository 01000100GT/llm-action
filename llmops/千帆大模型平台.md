



## 大模型训练相关

大模型训练是本平台的核心功能，统一查看模型的训练状态选择更加适合的模型训练方式。

| 概念名                                                                                                                   | 描述                                                                                    |
| --------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |
| 训练轮次                                                                                                                  | Epoch，训练轮次可以由步长 x 数据批大小/数据量的形式换算。例如，1w条样本在数据批大小为32的情况下，建议训练轮次至少设置为2。                  |
| 数据批                                                                                                                   | Batch_size，即一次训练所抓取的数据样本数量，Batch_size大小影响训练速度和模型的优化。                                  |
| 学习率                                                                                                                   | Learning rate （LR），是在梯度下降的过程中更新权重时的超参数，过高会导致模型难以收敛，过低则会导致模型收敛速度过慢，平台已给出默认推荐值，可根据经验调整。 |
| 全量更新                                                                                                                  | 训练过程中对大模型的全部参数进行更新。                                                                   |
| Prompt Tuning                                                                                                         | 在固定预训练大模型本身的参数的基础上，增加prompt embedding参数，并且训练过程中只更新prompt参数。                           |
| LoRA                                                                                                                  | 在固定预训练大模型本身的参数的基础上，在保留自注意力模块中原始权重矩阵的基础上，对权重矩阵进行低秩分解，训练过程中只更新低秩部分的参数。                  |
| [奖励模型](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Bliu6p62v#%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83) | 训练的最终目的是刻画模型的输出是否在人类看来表现不错。                                                           |
| [强化学习](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Bliu6p62v#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83) | 再励学习、评价学习或增强学习，利用问题集质问奖励模型，合成问题最优解的应答。    |




## 模型训练

### Baichuan2-7B-Chat

单条数据支持4096 tokens。Baichuan2-7B-Chat 是在大约 1.2 万亿 tokens 上训练的 70 亿参数模型。

| 训练方法 | 简单描述                                                                 |
| ---- | -------------------------------------------------------------------- |
| 全量更新 | 全量更新在训练过程中对大模型的全部参数进行更新                                              |
| LoRA | 在固定预训练大模型本身的参数的基础上，在保留自注意力模块中原始权重矩阵的基础上，对权重矩阵进行低秩分解，训练过程中只更新低秩部分的参数。 |

-   参数配置

| 超参数         | 简单描述                                                                                 |
| ----------- | ------------------------------------------------------------------------------------ |
| 迭代轮次        | 迭代轮次（epoch），控制训练过程中的迭代轮数。                                                            |
| 批处理大小       | 批处理大小（Batchsize）表示在每次训练迭代中使用的样本数。较大的批处理大小可以加速训练，但可能会导致内存问题。                          |
| 学习率         | 学习率（learning_rate）是在梯度下降的过程中更新权重时的超参数，过高会导致模型难以收敛，过低则会导致模型收敛速度过慢，平台已给出默认推荐值，可根据经验调整。 |
| 学习率调整计划     | 用于调整训练中学习率的变动方式。                                                                     |
| 学习率预热步数占比   | 指训练初始阶段，在学习率较低的情况下逐渐增加学习率的比例或速率，能够帮助模型更好地适应数据，提高训练的稳定性和性能。                           |
| 权重衰减数值      | 是一种正则化技术，用于帮助控制神经网络模型的复杂性以及减少过拟合的风险。                                                 |
| loraRank    | 训练方式选择**LoRA**时填写，LoRA策略中rank，数值越大lora参数越多。                                          |
| loraAlpha   | 训练方式选择**LoRA**时填写，LoRA微调中的缩放系数，系数越大lora影响力越大。                                        |
| loraDropout | 训练方式选择**LoRA**时填写，LoRA微调中的Dropout系数，用于防止lora训练中的过拟合。                                 |
| 序列长度        | 单条样本的最大长度。如果训练数据较短，减少此项可以加快训练速度。|



## 模型评估

### 自动规则打分指标[](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Bliu6x38y#%E8%87%AA%E5%8A%A8%E8%A7%84%E5%88%99%E6%89%93%E5%88%86%E6%8C%87%E6%A0%87)

| 指标名称        | 指标说明                                                                |
| ----------- | ------------------------------------------------------------------- |
| 准确率 (%)     | 规则打分模式下，忽略停用词后，正确预测(标注与预测完全匹配)的样本数与总样本数的比例                          |
| F1分数 (%)    | 规则打分模式下，忽略停用词后，精确率和召回率的调和平均数                                        |
| ROUGE-1 (%) | 忽略停用词后，将模型生成的结果和标准结果按unigram拆分后，计算出的召回率                             |
| ROUGE-2 (%) | 忽略停用词后，将模型生成的结果和标准结果按bigram拆分后，计算出的召回率                              |
| ROUGE-L (%) | 忽略停用词后，衡量了模型生成的结果和标准结果的最长公共子序列，并计算出召回率                              |
| BLEU-4 (%)  | 忽略停用词后，用于评估模型生成的句子和实际句子的差异的指标，值为unigram，bigram，trigram，4-grams的加权平均 |

**注释**：  
Ⅰ) unigram：指将句子或文本中的每个单词都单独作为一个基本单元，不考虑单词之间的顺序。  
Ⅱ) bigram：指将句子或文本中的每个相邻的单词对都作为一个基本单元，用于描述两个单词之间的顺序关系。  
Ⅲ) trigram：指将句子或文本中的每个相邻的三个单词作为一个基本单元，用于描述三个单词之间的顺序关系。  
Ⅳ) 4-grams：指将句子或文本中的每个相邻的四个单词作为一个基本单元，用于描述四个单词之间的顺序关系。  
Ⅴ) 最长公共子序列：指两个或多个字符串最长的子序列，这些子序列在每个字符串中都存在，且它们的顺序相同。


