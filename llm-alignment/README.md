



- [关于Instruct GPT复现的一些细节与想法](https://zhuanlan.zhihu.com/p/609078527)
- [人人都能看懂的PPO原理与源码解读](https://zhuanlan.zhihu.com/p/677607581)
- [MOSS-RLHF](https://github.com/OpenLMLab/MOSS-RLHF)
- [模型调优（RLHF/DPO/ORPO）- 终极指南](https://zhuanlan.zhihu.com/p/692594519)
- [DPO: Direct Preference Optimization 论文解读及代码实践](https://zhuanlan.zhihu.com/p/642569664)
- [强化学习入门：基本思想和经典算法](https://imzhanghao.com/2022/02/10/reinforcement-learning/)
- [动手学强化学习](https://hrl.boyuai.com/chapter/intro)


ORPO：
- ORPO: Monolithic Preference Optimization without Reference Model
- https://github.com/xfactlab/orpo
- https://arxiv.org/pdf/2307.12966.pdf


