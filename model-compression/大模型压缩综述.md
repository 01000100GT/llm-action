

- A Survey on Model Compression for Large Language Models
- https://arxiv.org/pdf/2308.07633.pdf

## 摘要


## 介绍


## 方法


### 剪枝

剪枝是一种强大的技术，通过删除不必要的或冗余组件来减少模型的大小或复杂性。众所周知，有许多冗余参数对模型性能几乎没有影响，因此在直接剪掉这些冗余参数后，模型性能不会受到太多影响。同时，剪枝可以在模型存储、内存效率和计算效率等方面更加友好。

剪枝可以分为非结构化剪枝和结构化剪枝，二者的主要区别在于剪枝目标和由此产生的网络结构。

结构化剪枝剪掉基于特定规则的连接或分层结构，同时保留整体网络结构。非结构化剪枝针对单个参数，会导致不规则的稀疏结构。最近的研究工作致力于将 LLM 与剪枝技术相结合，旨在解决与 LLM 相关的大规模和计算成本。



**结构化剪枝**：

- LLM-Pruner 

https://github.com/horseee/LLM-Pruner (300+)



**非结构化剪枝**：

- SparseGPT

https://github.com/IST-DASLab/sparsegpt (400+)

- LoRAPrune


- Wanda

https://github.com/locuslab/wanda (300+)




### 知识蒸馏

该技术将知识从被称为教师模型的复杂模型转移到被称为学生模型的更简单模型。KD 背后的核心思想是从教师模型的全面知识中转化出更精简、更有效的代表。本文概述了使用 LLM 作为教师模型的蒸馏方法。

研究者根据这些方法是否侧重于将 LLM 的涌现能力（EA）蒸馏到小模型（SLM）进行分类。

因此，这些方法被分为两类：标准 KD 和基于 EA 的 KD




**Standard KD**:

使学生模型学习教师模型(LLM)所拥有的常见知识，如输出分布和特征信息，这种方法类似于传统的KD。


- MINILLM

微软

https://github.com/microsoft/LMOps/tree/main/minillm

- GKD

Google DeepMind

**EA-based KD**:

不仅仅是将LLM的常见知识转移到学生模型中，还涵盖了蒸馏它们独特的涌现能力。具体来说，EA-based KD又分为了上下文学习（ICL）、思维链（CoT）和指令跟随（IF）。


In-Context Learning：

- In-context Learning Distillation

In-context Learning Distillation: Transferring Few-shot Learning Ability
of Pre-trained Language Models


哥伦比亚大学


Chain-of-Thought：

- MT-COT 

Explanations from Large Language Models Make Small Reasoners Better


加州大学圣塔芭芭拉分校、腾讯人工智能实验室、微软等

- Fine-tune-CoT 



- DISCO 

https://github.com/eric11eca/disco  (12)

DISCO: Distilling Counterfactuals with Large Language Models

洛桑联邦理工学院自然语言处理实验室、艾伦人工智能研究所


- SCOTT 

SCOTT: Self-Consistent Chain-of-Thought Distillation

南加州大学计算机科学系，亚马逊公司

- SOCRATIC CoT

苏黎世联邦理工学院计算机科学系



---


Instruction Following：

- Lion

Lion: Adversarial Distillation of Closed-Source Large Language Model


香港科技大学

https://github.com/YJiangcm/Lion  （144）
 


### 量化



### 低秩因式分解


- ZeroQuant-FP（低秩分解+量化）
- LoRAPrune（低秩分解+剪枝）




## 指标与基准

### 指标


LLM 的推理效率可以使用各种指标来衡量。这些指标考虑了性能的不同方面，通常与全面评估 LLM 的准确性和零样本学习能力一起呈现。

这些指标包括如下：

参数规模​
模型规模
压缩比
推理时间
浮点运算（FLOP）

### 基准（Benchmarks）

基准旨在与未压缩的 LLM 相比，衡量压缩 LLM 的有效性、效率和准确性。这些基准通常由不同的任务和数据集组成，涵盖了一系列自然语言处理挑战。常用基准包括但不限于 HULK 和 ELUE。




## 挑战



## 结论










