


PEQA

参数高效的微调（PEFT）方法已经出现，以减轻全面微调大型语言模型（LLM）的高昂成本。 尽管如此，法学硕士的庞大规模阻碍了常规部署。 为了解决这个问题，我们提出了参数高效和量化感知适应（PEQA），这是一种新颖的量化感知 PEFT 技术，可以促进模型压缩并加速推理。 PEQA 通过双阶段过程运行：最初，
每个全连接层的参数矩阵经过量化为低位整数矩阵和标量向量； 随后，对每个下游任务的标量向量进行微调。 这种策略大大压缩了模型的大小，从而降低了部署时的推理延迟并减少了所需的总体内存。 同时，快速微调和高效的任务切换成为可能。 通过这种方式，PEQA 提供了量化的好处，同时继承了 PEFT 的优点。 我们比较
PEQA 在从自然语言理解到生成基准的综合实验中具有竞争性基准。

这是使用多达 650 亿个参数的大型语言模型完成的，展示了 PEQA 的可扩展性、特定于任务的适应性能以及遵循指令的能力，即使在极低位设置下也是如此。