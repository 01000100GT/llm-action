
## 摘要

多种训练后量化方法已应用于大语言模型 (LLM)，并且已被证明在低至 8 比特的情况下也能表现良好。

我们发现这些方法在较低比特精度下会崩溃，并研究了 LLM 的量化感知训练(LLM-QAT) ，以进一步提高量化水平。

我们提出了一种 data-free 蒸馏方法，该方法利用预训练模型产生的生成，可以更好地保留原始输出分布，并允许独立于其训练数据来量化任何生成模型，类似于训练后量化方法。

除了量化权重和激活之外，我们还量化 KV 缓存，这对于提高吞吐量和支持当前模型大小的长序列依赖关系至关重要。

我们在低至 4 比特的量化级别上对大小为 7B、13B 和 30B 的 LLaMA 模型进行了实验。 我们观察到比 training-free 方法有很大的改进，特别是在低比特设置中。

## 1 Introduction







常识推理任务



我们考虑三种训练后量化（PTQ）方法：round-to-nearest（RTN）、GPT-Q 和 SmoothQuant 作为基线。 
我们在几种不同的设置中与它们进行比较，其中：权重、激活和 KV 缓存值被量化到不同的级别（表示为 W-A-KV）。 

不同的 PTQ 方法在不同的设置中表现良好，我们将我们的方法与每个设置中的最佳 PTQ 结果进行比较。


## 结论

我们提出了针对 LLM 的 data-free 量化感知训练，并表明使用该技术可以实现准确的 4 位量化。

考虑到与训练数据无关的蒸馏方法的普遍性，以及 LLM 部署成本的不断增长，我们期望我们的方法具有广泛的适用性。 
例如，该方法还可以用于在多个阶段训练的模型，例如 通过指令精调或强化学习。 

我们将这项调查留给未来的工作。 由于 4 比特量化没有开箱即用的硬件支持，因此，我们没有将硬件实现作为这项工作的一部分。 
不过，我们正在与合作伙伴合作，以期在不久的将来实现这一目标。 虽然我们的方法适用于 4 比特权重、4 比特 KV 缓存和 8 比特激活，
但对于 4 比特激活量化来说还不充分，还需进一步研究。
