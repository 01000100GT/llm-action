




LLM.in8() 论文中发现：激活中存在一些离群值，它们的绝对值明显更大；并且这些离群值分布在少量的几个特征 (features) 中，称为离群特征 (Emergent Features)。



不论是 per-token 还是 per-channel quantization，都会受到这些离群值的很大影响。










- https://github.com/TimDettmers/bitsandbytes


本质上，LLM.int8() 通过三个步骤完成矩阵乘法计算:

1. 从输入的隐含状态中，按列提取异常值 (即大于某个阈值的值)。
2. 对 FP16 离群值矩阵和 Int8 非离群值矩阵分别作矩阵乘法。
3. 反量化非离群值的矩阵乘结果并其与离群值矩阵乘结果相加，获得最终的 FP16 结果。


## 离群特征的重要性
超出某个分布范围的值通常称为离群值。离群值检测已得到广泛应用，在很多文献中也有涉及，且获取特征的先验分布对离群值检测任务很有助益。
更具体地说，我们观察到对于参数量大于 6B 的 transformer 模型，经典的量化方法会失效。
虽然离群值特征也存在于较小的模型中，但在大于 6B 的 transformer 模型中，我们观察到几乎每层都会出现超出特定阈值的离群点，
而且这些离群点呈现出一定的系统性模式。


如前所述，8 位精度的动态范围极其有限，因此量化具有多个大值的向量会产生严重误差。
此外，由于 transformer 架构的固有特性，它会将所有元素互相关联起来，这样的话，这些误差在传播几层后往往会混杂在一起。
因此，我们发明了混合精度分解的方法，以对此类极端离群值进行有效量化。



## MatMul 内部
计算隐含状态后，我们使用自定义阈值提取离群值，并将矩阵分解为两部分，如上所述。
我们发现，以这种方式提取所有幅度大于等于 6 的离群值可以完全恢复推理精度。
离群值部分使用 FP16 表示，因此它是一个经典的矩阵乘法，
而 8 位矩阵乘法是通过使用向量量化将权重和隐含状态分别量化为 8 位精度 - 即按行量化权重矩阵，并按列量化隐含状态，然后再进行相应向量乘加操作。
最后，将结果反量化至半精度，以便与第一个矩阵乘法的结果相加。

![](https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/Matmul.png)






