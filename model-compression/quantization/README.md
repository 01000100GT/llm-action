
## 简介

- https://docs.nvidia.com/deeplearning/tensorrt/tensorflow-quantization-toolkit/docs/docs/qat.html
- https://github.com/HuangOwen/Awesome-LLM-Compression



## Post Training Quantization(PTQ)

- ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers
  - https://www.deepspeed.ai/tutorials/model-compression/
  - 集成在Deepspeed
- SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
  - https://github.com/mit-han-lab/smoothquant
  - 已经集成在[FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- GPTQ: Accurate Post-training Compression for Generative Pretrained Transformers
- AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
  - https://github.com/mit-han-lab/llm-awq

## Quantization Aware Training（QAT）

- LLM-QAT: Data-Free Quantization Aware Training for Large Language Models
  - https://github.com/facebookresearch/LLM-QAT


