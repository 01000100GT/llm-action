


-----------------------

优化器。

为了实现梯度下降更快的收敛速度，经典的解决方案是融合动量技术，其中每一步都是最速下降方向和最近迭代位移的组合，有助于加速相关方向的梯度下降并抑制振荡。

开创性的工作包括Nesterov针对凸优化的加速梯度[67]和针对非凸问题的带有动量的近端梯度[55]等。

为了满足机器学习模型大规模优化的需求，主导优化器被设计为随机的。时尚。
特别是，动量随机梯度下降（SGD）和自适应学习率估计方法 Adam [50] 被广泛用于训练深度神经网络。

根据经验，使用 Adam 训练 Transformer 的性能优于 SGD，并且 [96] 揭开了随机梯度中噪声的重尾分布是 SGD 性能不佳的主要原因的神秘面纱，并通过自适应噪声裁剪的视角来理解 Adam。

默认情况下，AdamW [62] 是 Adam 的一个变体，它解耦了 L 正则化和权重衰减，是 Transformer 使用最广泛的优化器。

最近，Google 搜索优化算法并发现了一种简单有效的优化器，称为 Lion [18]。 Lion仅跟踪一阶梯度的动量，其更新仅考虑符号方向并且每个参数具有相同的幅度，这与AdamW等自适应优化器有很大不同。在实践中，Lion 通常收敛速度更快，并且在各种基准上训练 Transformer 时比 AdamW 更节省内存且更准确。 我们建议读者参考 [60, 9]，了解有关机器学习中加速优化方法的更多详细信息。


为了提高 Transformers 的泛化能力，锐度感知最小化（SAM）[28]寻求基于损失景观的几何形状与泛化之间的联系，同时最小化损失值和损失锐度，即，更平坦的最小值往往会提高泛化能力。以下工作 [17] 将 SAM 应用到 Transformer，通过平滑损失表面观察到显着的精度增益。然而，SAM 需要解决双层最小-最大优化问题，这几乎使训练时间增加了一倍。

为了加速优化，[23]提出了随机权重扰动保留泛化能力和锐度敏感的子集选择策略。最近，[24]通过将锐度估计替换为两个连续更新步骤之间的 KL 散度，设计了锐度损失的近零成本代理。



初始化。

良好的初始化对于稳定训练、提高学习率、加速收敛和提高泛化能力至关重要。

因此，为了更好地初始化 Transformer，人们提出了许多工作。
具体来说，Fixup [95]建议适当地重新调整标准初始化，以确保适当的梯度范数，以避免梯度爆炸或消失，这可以训练具有超过 10,000 层的非常深的网络，而无需添加归一化层。
基于归一化残差块计算的函数接近恒等函数（即单位方差）的认识，以下工作ReZero [5]和SkipInit [22]简单地初始化每个层以执行恒等操作。具体来说，他们在每个残差块的输出上添加了一个可学习的缩放乘数：


针对 Transformers 定制的 T-Fixup [40] 分析认为，部分优化难度来自于 Adam 优化器的早期更新不稳定，因为二阶动量的方差是无界的。因此，跟随Fixup采用重缩放方案来初始化残差块。所有上述方法都从所有块中删除了批量/层归一化，并且在没有学习率预热的情况下进行训练。在训练深度视觉 Transformers（ViT）时，[85]提出了通道可学习的缩放因子，并根据经验观察到重新引入预热和层归一化技术可以使训练更加稳定。










稀疏训练。

稀疏训练的关键思想是直接训练稀疏子网络而不是从头开始训练完整网络而不牺牲准确性。

可靠性首先由彩票假设（LTH）[29] 证明，即密集、随机初始化的网络包含子网络（中奖彩票），可以单独训练这些子网络以匹配原始网络的准确性。然而，LTH 需要以交替训练-剪枝-再训练的方式识别中奖彩票，这使得大型模型和数据集的训练成本极高，限制了实际收益。

有鉴于此，后续训练效率较高的工作可以大致分为三类：
（i）在初始化时通过测量连接对损失的重要性来找到稀疏网络，从而消除了复杂的迭代优化计划的需要[ 52、88]； 
(ii) 通过低成本方案在非常早期的训练阶段识别《变形金刚》中的中奖彩票，然后仅训练这些早期彩票直到收敛 [90, 16]；
 (iii) 使用交替修剪和生长计划在整个训练过程中动态更新模型稀疏模式，适用于一般架构 [26, 14]。


过度参数化。

实际的 DNN 严重过度参数化，可学习参数的数量远大于训练样本的数量。据观察，过参数化在经验上提高了收敛性和泛化性，虽然有理论保证，但还不够。

早期的工作 [4] 从数学上证明，线性神经网络中过度参数化增加深度可以加速 SGD 收敛。 

[57]进一步探索了两层非线性神经网络，[3]证明SGD可以在多项式时间内收敛到DNN训练目标上的全局最小值，假设训练样本不重复，并且参数数量是多项式训练样本的数量和网络深度。

在泛化方面，[2]从理论上证明了一个充分超参数化（三层）的神经网络可以泛化到总体风险，并且一个有趣的特性是在 SGD 训练轨迹上的任何点的近邻中都存在一个精确的网络随机初始化的概率较高。请注意，它与 LTH 有着深厚的联系，因为它部分解释了为什么 LTH 处于稀疏训练中，因为由于过度参数化，具有低风险的良好小型子网络非常丰富。

应用于变形金刚，[58]利用了更快的收敛和更好的泛化

将过参数化理论应用于 Transformer 设计高效的训练流程（训练一个非常大的模型，然后执行早期停止并对其进行大量压缩，类似于 LTH）获得了更快的收敛和更好的泛化



大批量训练。加速训练的另一种流行方法是使用大批量，减少每个时期的迭代次数并提高计算资源利用率。从统计角度来看，大批量训练减少了随机梯度估计的方差，因此需要调整可靠的步长以获得更好的收敛性[9]。在卷积神经网络时代，[31]采用学习率的线性缩放在ImageNet上训练ResNet-50，批量大小为1小时8,192。随后提出了更先进的步长估计方法。广泛使用的方法是针对 SGD 的 LARS [92] 和针对 Adam 的 LAMB [93]，它们建议分别对 ResNet 和 Transformers 使用分层自适应学习率。分层适应策略可以表示为




其中 η、wan 和 γ 是时间步 t 时第 i 层的学习率、参数和基于动量的梯度，φ 是缩放函数。它配备了一个归一化项，可以为爆炸梯度和平台提供鲁棒性，并且缩放项确保更新的范数与参数的范数具有相同的阶数，从而促进更快的收敛。最近，经验表明，针对大批量训练定制的更强大的优化方法表现良好。例如，[46]表明，对一定数量的最新检查点的权重进行平均可以促进更快的训练。 [37] 中的 DeepMind 训练了超过 400 个具有不同规模的模型大小和训练标记数量的 Transformer 语言模型，达到了一个实际假设，即模型大小和训练标记的数量应该同等缩放以获得计算最优<b1001>< /b1001> 培训。




增量学习。增量学习的高级概念是将原始具有挑战性的优化问题放松为一系列易于优化的子问题，其中一个子问题的解决方案可以作为后续子问题的良好初始化，以规避训练难度，与退火类似。一些工作 [30, 32] 提出通过逐步堆叠层、正确地从较小的模型初始化较大的模型来加速 BERT 预训练。 [97]以相反的方向通过层下降来训练随机深度的 Transformer，它沿着时间维度和深度维度逐渐增加下降率。 AutoProg [54] 针对 ViT 进行了定制，建议使用神经架构搜索来自动决定模型在渐进学习过程中是否应该增长、在何处增长以及增长多少。一个关键的观察结果是，逐步提高输入图像的分辨率（减小 patch 大小）可以显着加速 ViT 训练，这与众所周知的训练动态相一致，该训练动态侧重于早期阶段的低频结构和后期阶段的高频语义。后期。




--------------


令牌屏蔽。标记掩码是自监督预训练任务中的主要方法，例如掩码语言建模（MLM）[48, 10]和掩码图像建模（MIM）[6, 35]。

标记屏蔽的本质是随机屏蔽一些输入标记，并训练模型使用可见标记的上下文信息来预测丢失的内容，例如词汇 ID 或像素。由于压缩序列长度可以成倍地降低计算和内存复杂性，因此跳过处理屏蔽标记可以为 MLM 和 MIM 带来可观的训练效率增益。

对于 MLM，[80] 提出联合预训练编码器和解码器以执行语言生成任务，同时删除解码器中的屏蔽标记以节省内存和计算成本。

对于 MIM，代表性工作 [35] 表明，在视觉中，在编码器之前删除屏蔽图像块表现出更强的性能，并且比保留屏蔽标记的总体预训练时间和内存消耗低 3 倍或更多。在[56]中也发现了类似的现象，对于语言图像预训练，随机屏蔽和删除屏蔽图像块显示出比原始CLIP[72]快3.7倍的整体预训练时间。



重要性抽样。对数据的重要性采样，也称为数据剪枝，理论上可以通过优先考虑信息丰富的训练样本来加速监督学习的随机梯度算法，主要受益于方差减少。

对于 DNN，估计每个样本重要性的主要方法是使用梯度范数，并且 [47, 44] 使用不同的近似值
[47] 使这些规范的计算变得容易处理。 [70]进一步加速了类似于早鸟LTH的采样过程，但在数据域中，几个权重初始化上的简单平均梯度范数或误差范数可以用于在早期阶段识别重要的例子。训练。最近，[81]展示了一个令人兴奋的分析理论，即如果配备了卓越的数据修剪指标，测试误差随数据集大小的缩放可以突破幂缩放定律，并且至少可以减少到指数缩放，并且它采用了自我监督使用 k 均值聚类的度量。它展示了基于数据重要性采样的更有效的神经缩放法则的有前途的方向。






-----------------------


内存高效：



对于DP来说，随着可用worker的增加，batch size接近线性缩放。

DP 具有较高的通信/计算效率，但当模型变大时，内存效率较差，单个设备无法存储模型副本，并且梯度的同步通信会阻碍 DP 的可扩展性。
因此，DP本身只适合训练中小型模型。为了提高DP的可扩展性，Transformer的一种解决方案是参数共享，称为Albert，但它限制了表示能力。


最近，ZeRO 将统一分区策略与 DP 结合起来，其中每个数据并行过程仅处理模型状态的一个分区，在混合精度状态下工作。为了处理非常大的 DNN，人们总是需要利用模型并行以“垂直”方式在多个加速卡上分配不同的层。

尽管MP具有良好的存储效率，但由于跨设备的大量数据传输和PE利用率较差，其通信和计算效率较低。幸运的是，有两种策略可以进一步提高 MP 效率。

张量并行（TP）和管道并行（PP）。 

TP 将张量操作划分为跨worker的张量操作，以加快计算速度并节省更多内存。 

就PP而言，它最初是在GPipe中提出的，它将输入的小批量分割成多个更小的微批量，使得不同的加速卡在应用之前同时处理不同的微批量。对于整个小批量使用单个同步梯度更新。然而，它仍然受到流水线气泡（加速器空闲时间）的影响，从而降低了效率。

特别是，PyTorch 实现了 torchgpipe ，它通过检查点执行微批次 PP，允许扩展到大量微批次以最小化气泡开销。




同时量化激活/权重/梯度降低精度训练之外，
激活压缩训练（ACT）在精确计算前向传递时存储激活的低精度近似副本，这有助于减少训练期间的总体内存消耗。 然后，保存的激活在向后传递中被反量化到原始精度以计算梯度。



重计算 与 Offload。再物化，也称为检查点[15]，是一种广泛使用的时空权衡技术，它仅在前向传递期间存储一部分激活/权重，并在后向传递期间重新计算其余部分。 [15]提供了一个在 PyTorch 中实现的简单周期调度，但它仅对于同质顺序网络是最佳的。更先进的方法（例如[36]）实现了异构网络的最佳检查点。就卸载而言，它是一种利用CPU内存等外部内存作为GPU内存的扩展，通过GPU和CPU之间的通信来增加训练过程中内存容量的技术。模型状态和激活可以卸载到 CPU，但最佳选择需要最大限度地减少与 GPU 之间的通信成本（即数据移动）、减少 CPU 计算并最大限度地节省 GPU 内存。

一个代表性的工作是 ZeRO-Offload [75]，它提供了使用 Adam 优化器针对混合精度训练定制的最佳卸载策略。它将所有 fp32 模型状态和 fp16 梯度卸载到 CPU 内存上，并在 CPU 上计算 fp32 参数更新。 fp16参数保存在GPU上，前向和后向计算都在GPU上。为了两全其美，[7]建议联合优化激活卸载和重新实现。



参数高效调整。以HuggingFace为代表的公共模型动物园包含丰富的预训练模型，可供下载和执行，为降低训练成本做出了巨大贡献。

高效微调这些现成的模型正在成为大幅降低训练成本的流行方法。作为原版完全微调的强大替代方案，参数高效调整（PET）仅更新少量额外参数，同时冻结预训练模型以显着减少存储负担，可根据动态部署场景进行扩展，而无需为每种情况存储单独的模型实例。

一般的 PET 方法可以分为基于加法的方法和基于重新参数化的方法。前者将额外的可训练参数附加到预训练模型上，并且仅调整这些参数。例如，[53, 43] 将可训练参数添加到输入空间，[38] 在 MSA 和 FFN 之后将适配器模块添加到每个 Transformer 块中两次。然而，额外的参数在推理过程中引入了额外的计算和内存开销。

为了应对这一挑战，后者建议调整模型[94]中固有的参数或可以重新参数化到模型[39]中的新参数，从而不会牺牲推理效率。受大型语言预训练模型内在维度较低的观察启发[1]，代表性工作LoRA[39]将自注意力权重的更新近似为两个低秩矩阵，可以在推理过程中合并到预训练权重中。

值得注意的是，最受认可的民主化努力之一是斯坦福羊驼[83]，它是使用从ChatGPT生成的52K指令跟踪数据对开源LLaMA模型[86]进行微调的。为了廉价且高效地对其进行微调，其变体 Alpaca-LoRA 进一步采用低阶 LoRA 来支持在客户硬件上指令调整 LLaMA，表明可以在单个 RTX 4090 上在数小时内完成训练。





开源框架：Microsoft DeepSpeed、HPC-AI Tech Colossal-AI、Nvidia Megatron-LM


DeepSpeed 主要基于[74]和ZeRO系列作品[73, 75]实现。

Colossal-AI 基于（Colossal-ai: A unified deep
learning system for large-scale parallel training）构建。

Megatron-LM 基于 （Efficient large-scale language model
training on gpu clusters using megatron-lm）实现。

所有这些都支持混合精度的数据和模型并行，以及卸载和重计算等其他通用实践。

更多用于高效分布式训练的库包括但不限于 HuggingFace Transformers、MosaicML Composer、百度 PaddlePaddle、Bytedance Lightseq、EleutherAI GPT-NeoX 等。



----------------------












----------------------

硬件/算法协同设计

除了计算和内存负担之外，设计高效的硬件加速器还可以加快 DNN 的训练和推理速度。具体来说，与中央处理单元（CPU）相比，图形处理单元（GPU）由于并行度高，执行矩阵乘法的能力更强大。对于专注于特定计算任务的应用，专用集成电路（ASIC）具有低功耗和高训练/推理速度的优势。例如，谷歌设计的张量处理单元（TPU）的每瓦性能比当代 CPU 和 GPU 高 30∼80 倍 [45]。然而，ASIC 不容易重新编程或适应新任务。相比之下，现场可编程门阵列 (FPGA) 旨在根据需要重新编程以执行不同的功能，并且还可以在最终设计之前用作 ASIC 的原型。

为了进一步优化 DNN，特别是 Transformer 的训练效率，硬件算法协同设计在设计算法时考虑了硬件的约束和能力，这将在下面的小节中介绍。

---

稀疏矩阵乘法。为了减少 Transformers 的计算开销，稀疏通用矩阵乘法（SpGEMM）涉及将稀疏矩阵与稠密矩阵相乘，利用注意力矩阵的稀疏性来减少计算量。

有几种流行的稀疏矩阵计算库，例如 Intel Math Kernel Library  on CPU 和 cuSPARSE、CUSP 以及 GPU 上的 2:4 结构化稀疏性 [65]。

然而，由于不规则的稀疏性，SpGEMM 通常对通用处理器（例如 CPU 和 GPU）的硬件不友好。

为了解决这个问题，需要专门的硬件加速器（例如 FPGA 和 ASIC）来处理数据局部性较差的问题。例如，OuterSPACE [68]将矩阵乘法转换为外积过程，并通过将乘法与累加解耦来消除冗余内存访问。

为了在不引入大量开销的情况下充分利用这种优势，OuterSPACE 构建了一个具有可重新配置内存层次结构的定制加速器，与运行英特尔数学核心库的 CPU 相比，平均加速达到 7.9 倍，与运行 CUSP 的 GPU 相比，平均加速达到 14.0 倍。

此外，为了缓解高稀疏性引起的数据移动瓶颈，ViTCoD [91]使用可学习的自动编码器将稀疏注意力压缩为更紧凑的表示，并设计编码器和解码器引擎以提高硬件利用率。

---

硬件感知的低精度。降低计算精度会减少内存和计算量，这可以以硬件友好的定点或整数表示形式（而不是浮点表示形式）来实现。因此，我们可以使用较低精度的乘法器、加法器和内存块，从而显着改善功耗和加速。此外，低精度算法可以与剪枝和低秩近似等其他技术相结合，以实现进一步的加速。

例如，Sanger [63] 使用 4 位Query和Key来计算稀疏注意力矩阵的量化预测。然后，稀疏注意力掩模被重新排列成结构化块并由可重新配置的硬件处理。

以下工作 DOTA [71] 使用低秩变换和低精度计算来识别注意力中不重要的连接。通过结合令牌级并行性和乱序执行，DOTA 比 GPU 实现了 152.6 倍的加速。

---


高效的注意力。除了稀疏矩阵乘法和低精度计算之外，一些开创性的工作还专注于硬件中高效且轻量级的注意力实现[33,34,20]。

具体来说，A[33]仅选择那些可能与给定查询具有较高相似度的键，以减少注意力的计算量。 

ELSA [34]根据哈希相似性过滤掉特定查询的不相关键以节省计算。凭借高效的硬件加速器，与配备 16GB 内存的 Nvidia V100 GPU 相比，ELSA 的加速速度提高了 58.1 倍，能源效率提高了三个数量级。

值得注意的是，FlashAttention [20] 提出利用平铺来减少 GPU 高带宽内存 (HBM) 和片上 SRAM 之间的 I/O 通信，这正在成为默认的快速且内存高效的注意力块，以实现加速。














