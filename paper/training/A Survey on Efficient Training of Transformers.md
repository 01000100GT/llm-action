

涵盖了加速算法和硬件方面的最新进展，
分析和比较了在训练期间节省中间张量的计算和内存成本的方法，以及硬件/算法协同设计的技术。



## Introduction




 SOTA 大规模模型需要内存高效的训练技术，以减少存储中间张量和跨加速卡的数据交换（通信）的内存占用，同时确保高处理元件利用率。



计算优化


Optimization

优化器

初始化



稀疏训练


过参数化




大批量训练



数据选择

Token masking


重要性抽样


内存优化








## 硬件/算法协同设计








硬件感知的低精度。降低计算精度会减少内存和计算量，这可以以硬件友好的定点或整数表示形式（而不是浮点表示形式）来实现。因此，我们可以使用较低精度的乘法器、加法器和内存块，从而显着改善功耗和加速。

此外，低精度算法可以与剪枝和低秩近似等其他技术相结合，以实现进一步的加速。例如，Sanger [63] 使用 4 位 queries
and keys 来计算稀疏注意力矩阵的量化预测。然后，稀疏注意力masks被重新排列成结构化块并由可重新配置的硬件处理。以下工作 DOTA [71] 使用低秩变换和低精度计算来识别注意力中不重要的连接。通过结合Token-level 并行和乱序执行，DOTA 比 GPU 实现了 152.6 倍的加速。





高效的注意力。除了稀疏矩阵乘法和低精度计算之外，一些开创性的工作还专注于硬件中高效且轻量级的注意力实现 [ 33,
34 , 20 ]。具体来说，A[33]仅选择那些可能与给定queries具有较高相似度的keys，以减少注意力的计算量。 

ELSA [34]根据哈希相似性过滤掉特定query的不相关的keys以节省计算。凭借高效的硬件加速器，与配备 16GB 内存的 Nvidia V100 GPU 相比，ELSA 的加速速度提高了 58.1 倍，能源效率提高了三个数量级。值得注意的是，FlashAttention [20] 提出利用tiling来减少 GPU 高带宽内存 (HBM) 和片上 SRAM 之间的 I/O 通信，这正在成为默认的快速且内存高效的attention模块，以实现加速。









