

Attention（注意力机制）： Attention机制允许模型为输入序列中的每个位置分配不同的权重，用以关注输入序列中不同位置的信息。它通过计算每个位置与其他所有位置之间的相似度（通过点积、缩放点积等方法），然后将这些相似度转换成权重，最后将输入序列中的所有位置按照这些权重进行加权求和。这种机制使得模型能够处理长距离的依赖关系，同时能够并行计算，提高了模型的效率。

Feed-Forward Neural Network (FFN)（前馈神经网络）： 每个Transformer层都包含两个线性变换，之间由非线性激活函数（通常是ReLU）连接。FFN对每个位置的表示进行独立的变换，从而捕捉到位置特定的模式和特征。这个步骤有助于提高模型的非线性建模能力。

Layer Normalization（层归一化）： 在每个Transformer层的子层（Attention和FFN）之后都会应用LayerNorm。LayerNorm的作用是对每个位置的特征进行归一化处理，使得每个特征的均值接近0，标准差接近1。这样做有助于缓解训练时的梯度消失问题，并且可以加速训练过程。

Add & Normalize（加和与归一化）： 在每个子层（Attention和FFN）的输入和输出之间应用残差连接（或者称为skip connection），然后对输出进行LayerNorm操作。这个步骤的目的是引入残差连接，使得模型可以学习到输入和输出之间的差异，有助于减缓梯度消失问题，同时也使得模型更容易学习到恒等映射。在LayerNorm之后应用残差连接有助于稳定训练。


Attention机制用于捕捉输入序列中的关联关系，
FFN用于捕捉每个位置的非线性特征，从而增加模型的表示能力和拟合复杂模式的能力，
LayerNorm用于归一化特征并缓解梯度消失问题，而Add & Normalize结构引入残差连接，使得模型更容易训练。