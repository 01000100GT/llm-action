



由于先进的算法设计（例如 GPTQ [19] 和 LoRC [69]），质量下降大大减少]。

然而，这些进步主要集中在零样本评估上，可接受的质量下降是针对较大的模型尺寸（大于 13B），但它们通常需要对较小的模型尺寸（例如 1B）进行重大权衡。此外，他们只关注零样本测量[62, 69]。

在生产环境中，复制原始模型的不同任务的性能至关重要，模型质量的任何损失都是一个主要问题。
现有方法虽然具有创新性，但并不能完全满足在实际应用中部署LLMs的实际要求。




为了应对这些挑战，我们的贡献如下：


扩大评估范围和量化分析。我们的研究表明，现有的定量

像 GPTQ 这样的标准化方法往往会过度拟合校准数据集。更重要的是，我们扩大了 LLMs 中 4 位量化分析的范围，以包括零样本之外的任务，例如代码生成和抽象摘要。我们发现 INT4 量化通常表现不佳，尤其是在较小的模型中，甚至在参数高达 130 亿个的情况下，例如 LLaMA-13b。详细信息请参见第 3 节。




FP6 量化的卓越性能。我们说明了 FP6 采用基本的舍入最近 (RTN) 算法和粗粒度量化方法，始终达到与全精度模型相当的精度，在广泛的生成任务中证明了高度有效。具有 FP6 量化功能的 StarCoder-13B 模型在代码生成任务中的性能与其 FP16 等效模型相匹配。对于 406M 等较小型号，它与汇总中的基线结果密切相关。这些成就超出了 INT4 量化的能力。如需更深入的探索，请参阅第 4 节。




创新的4+2 FP6设计。我们为 FP6 引入了创新的 4+2 设计，克服了 AI 硬件上先前的集成和加速问题。该设计实现了与最先进的 INT4 细粒度量化类似的延迟，使 FP6 成为 LLMs 中现有 4 位量化方法的可行替代方案。详细信息请参见第 5 节。




都使用零样本困惑度或准确性等指标来评估量化的影响[66,19,8,2,29]。然而，鉴于LLMs的主要现实应用，例如ChatGPT[5]和Codex[21]，都是围绕基于生成的任务展开的，因此需要一个更全面的量化LLMs评估框架b1002> 是有保证的。


--------

评估：

Zero-Shot 评估 (Perplexity)



RTN。舍入到最近邻


GPTQ


最突出的是 SmoothQuant [66]、AWQ [38]、Quip [8]、 SqueezeLLM [ 29]、QUIK [ 2 ] 和 LLM-FP4 [ 40] 以及更多 [ 69, 14]。然而，这些方法通常需要使用额外的稀疏矩阵或额外的程序来精确定位敏感权重。

此外，这些研究大多数主要集中在零样本困惑度和准确性性能上[69,19,62]。然而，这些发现在多大程度上可以推广到其他生成任务仍有待充分探索。


通过三个指标评估性能：零样本任务、代码生成和总结。我们还尝试对那些基于聊天的模型进行比较实验，并根据 FastChat 代码通过 GPT-4 进行判断[75]。尽管如此，由于我们的研究结果存在显着差异，我们得出的结论是位精度和性能之间没有明确的联系。



三个标评估性能

Zero-Shot 任务 的 困惑度和准确性

生成任务（ROUGE  或Pass@1）

总结任务()



GPTQ 的过度拟合倾向。 尽管 GPTQ 在训练后量化方面具有创新性，但它往往会过度拟合特定的数据集，尤其是其细粒度量化结果。如表 1 所示，我们看到，如果我们使用特定数据集（例如 GPTQ 的 C4）进行校准，那么该 C4 数据集的性能会好得多（使用 FGQ 参见 9.34 或 6.74），而其他数据集（例如 PTB）会导致性能更差（




对于较大的模型（从 1B 移动到 13B 或 65B），过拟合现象不太严重。事实上，如表 2 所示，我们看到 LLaMA-65B 在 FGQ 上使用 GPTQ 作为 INT4 权重，与 RTN (7.17) 相比，得到最佳平均困惑度 6.61，更接近基线 6.41。然而，它在增强零样本性能方面的有效性有些有限（详见表3），这表明它在各种语言建模场景的适应性方面存在差距，并强调了模型评估中鲁棒性的必要性。

特别是，我们在表 3 中展示了 RTN 和 GPTQ 在 INT4 权重上的比较，同时保持激活不变，我们不能声称 GPTQ 和 RTN 比基于零样本性能的另一个更好。事实上，对于 LLaMA-65B，RTN 的性能比 FP16 的性能要好得多。



 LLMs的核心优势在于其生成序列的能力。

本文重点是评估摘要和代码生成，如表 4 所示。该策略强调了超越零样本学习的全面且详细的测试方法的重要性，旨在全面评估 LLMs 的生成能力。


表 4 中的数据显示 INT4 的性能存在显着差异，尤其是与标准基准测试相比。例如，Java-Script 中的 CodeLLaMA-34B 模型的性能从 45.05 (FP16) 下降到 43.45 (INT4, CGQ) 或 43.22 (INT4, FGQ)，分别下降了 1.6 和 1.83 个点。

虽然 INT4 上的 FGQ 比 CGQ 提供了相当大的改进，但与 FP16 相比仍然存在差距，特别是对于较小的模型和 Java 脚本。

有趣的是，FGQ 上的 INT4 CodeLLaMA-34B 在 Python 代码中得分为 46.88，超过了其基准，而 FGQ 上的 INT4 CodeGeeX2-6B 得分仅为 29.8，甚至落后于其 INT4-CGQ 性能。这凸显了 INT4 的不一致性。


这些结果强调需要研究 INT4 在复杂生成任务中的有效性。



其在代码生成和摘要任务中的不稳定性和低于标准的结果，本节深入探讨了浮点量化研究中的一个新兴领域。 最近的研究越来越关注使用浮点量化来处理LLMs内的权重或激活。



--------


基于之前围绕 INT4 量化相关挑战和局限性的讨论，特别是其在代码生成和摘要任务中的不稳定性和低于标准的结果，本节深入探讨了浮点量化研究中的一个新兴领域。最近的研究越来越关注使用浮点量化来处理LLMs内的权重或激活[62,40,74,44,7,32,58]。值得注意的是，简单的 FP8 在激活过程中的应用比 INT8 的使用显示出显着的改进 [62]。受这些进步的启发，出现了一个关键问题：提高位精度（例如提高到 5 位或 6 位）能否在生成任务中提供更稳定、更稳健的结果？本节旨在探讨 FP6（FP5）的有效性程度及其对不同量化算法的弹性，为之前 INT4 量化挑战带来的困境提供潜在的解决方案。
为了完整起见，我们提供浮点格式的简化概述。详细解释请参考[10]。标准浮点数由三部分组成：符号位、指数位和尾数位。这可以简化为：





为什么不用 INT6 而不是 FP6？

选择 FP6 而不是 INT6 是由两个关键因素驱动的：首先，FP 格式简化了转换过程，因为最终计算通常使用 FP16 或 BF16 执行。

其次，正如[62]中的研究结果所支持的，这些格式之间的准确性没有观察到差异，从而无需额外的实验验证。


使用pass@k指标评估功能正确性，即每个问题生成k个代码样本，如果有任何样本通过单元测试，则认为问题已解决，并报告问题解决的总比例。然而，以这种方式计算pass@k会有很高的方差性。相反，为了评估pass@k，我们为每个任务生成n≥k的样本（在本文中，我们使用n=200，k≤100），计算通过单元测试的正确样本c≤n的数量，并计算出无偏估计器。



因为我们没有发现位精度和性能之间存在明确的联系。




FP6 稳健性。 FP6 量化，尤其是 CGQ 的量化，展示了显着的进步，在各种任务和模型中几乎匹配 FP16 基线。
这种量化方法不仅缩小了 FP5 和 INT4 的性能差距，而且在处理不同任务时表现出鲁棒性。

在 FP6 框架内比较 CGQ 和 FGQ 时，鲁棒性得到进一步强调（因为有
CGQ 和 FGQ 之间差别不大），其中带有 CGQ 的 FP6 始终保持接近基线的高性能，表明其在不同场景下的有效性和稳定性。

此外，FP6 对量化算法具有鲁棒性：RTN 或 GPTQ 都会产生相似的结果，特别是对于 CodeLLaMA-34B，如表 6 所示。



------------


此方法与两种通常考虑的策略不同：

第一种方法涉及直接将 6 位格式转换为 8 位浮点 (FP8) 格式。虽然这是一个简单的解决方案，但不幸的是，它否定了 6 位格式的主要优点，即节省内存。

第二种方法需要将多个 6 位数字组合在一个连续的内存块中，并使用 32 位整数 (INT32) 或 32 位浮点 (FP32) 格式表示它们。该方法保持了节省内存的优势，但增加了反量化过程的复杂性。




我们独特的策略侧重于将 6 位数字划分为两个不同的子数字：第一个子数字代表最初的 4 位，第二个子数字代表剩余的 2 位。

我们提出的“4+2”方法可以看作是第二种标准方法的高级变体。 

4+2 位除法基于任何正整数都可以表示为 2 的幂和的基本原理。

4+2 位除法的基本原理是，任何正整数都可以表示为 2 的幂和。

在此基础上，我们将 6 位数字分为两个部分：


第一部分由最初的 4 位组成，处理符号位和 3 位指数等元素。

第二部分包含剩余的 2 位，专用于 2 位尾数。



这种划分为 4+2 位的方式有利于这些子数的同时加载和反量化，最终生成最终的 16 位浮点 (FP16) 权重。

我们的方法创新地平衡了减少内存占用的需求与反量化的实用性，特别是在解决跨分段数字内存访问的挑战方面。



偏置转移



在 GPU 上运行时将 FP6 反量化为 FP16 可能会占用大量资源，这主要是由于操作指数字段所涉及的复杂性，如第 4 节所述。





指数的偏置项通常由指数位确定，对于 FP6 为 3，对于 FP16 为 15。数学上，将FP6反量化为FP16（不包括符号）的过程表示为：





其中上标 FP16/FP6 表示各自的格式。

值得注意的是，缩放因子反量化可以在细粒度（子行）量化方案的累加之前的矩阵乘法之后进行，或者在粗粒度（行方式）量化方案的累加之后进行。

虽然填充（padding）可以轻松调整尾数，但由于偏置的差异，对齐指数需要更多的努力。可以通过将 INT4 数字转换回对称 INT8 格式来进行类比：如果 INT4 采用对称格式（对于尾数），则零填充就足够了。然而，在非对称格式中，单独填充是不够的，还需要额外的步骤。

为了解决这个问题，我们定制了 FP6 格式，其非标准指数偏置为 15。此修改不会影响精度或准确度，因为：



这意味着偏置偏移可以无缝集成到缩放因子中。至关重要的是，由于 Si 小于 1，因此将其与 2 相乘仍然可以通过简单的指数位移位以 FP16 格式准确表示，从而避免数值错误。


我们的偏置偏移方法极大地简化了运行时的 FP6-FP16 去量化过程。

为了证明这一点，我们在图 1a 和 1b 中进行了并排比较。

图 1a 概述了对每个 FP6 权重进行去量化的原始两步过程。第一步涉及将 W 转换为 W，第二步需要乘以量化比例 S。此过程中要求最高的部分是重新计算 W 的指数，其中涉及从 W 中提取 E，加上 12，然后将其合并回W. 此外，对次正规 FP6 数进行反量化的过程进一步增加了运行时反量化的复杂性。


然而，利用我们的偏置转移策略，如图 1b 所示，指数调整变成了一个简单的位级填充过程。最初在步骤 1 中需要添加常量整数 12，现在可以推迟到步骤 2，从而消除任何运行时开销。这是可能的，因为量化比例与常数整数的乘法可以在模型量化之后和运行时之前静态地执行。此外，这种简化的方法还有效地适应了subnormal数的反量化。




 FP6 内核通过 Bias-Shift 增强，速度比 cuBLAS 快 2.37 倍，平均快 1.92 倍。鉴于 LLM 推理通常受到有限的 GPU DRAM [64, 30] 的限制，我们的方法通过最小化模型权重内存访问有效地缓解了这一瓶颈。此外，我们的 FP6 内核在速度上优于最先进的细粒度 INT4 实现，第一前馈层 (FFN1) 平均快 1.06 倍，第二前馈层平均快 2.05 倍（ FFN2）。应该指出的是，图 2 仅提供了关键成果的快照，全面的系统实现和详细的性能分析为将来的工作保留。重要的是，我们带有 Bias-Shift 的 FP6 内核比没有 Bias-Shift 的相同 FP6 内核平均快 1.36 倍，这强调了 Bias-Shift 的关键作用，如第 5.2 节中讨论的那样。




奇数位 GPU 内核优化







