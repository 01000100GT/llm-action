
近年来，随着Transformer、MOE架构的提出，使得深度学习模型轻松突破上万亿规模参数，从而导致模型变得越来越大，因此，我们需要一些大模型压缩技术来降低模型部署的成本，并提升模型的推理性能。而大模型压缩主要分为如下几类：

-   剪枝（Pruning）
-   知识蒸馏（Knowledge Distillation）
-   量化（Quantization）
-   低秩分解（Low-Rank Factorization）

下面主要针对大模型蒸馏技术进行相应的讲解，本系列一共分六篇文章进行讲解。

- 大模型知识蒸馏原理综述（一）：概述
- 大模型知识蒸馏原理综述（二）：MINILLM、GDK
- 大模型知识蒸馏原理综述（三）：In-Context Learning distillation 
- 大模型知识蒸馏原理综述（四）：SCOTT、DISCO、MT-COT
- 大模型知识蒸馏原理综述（五）：Lion
- 大模型知识蒸馏原理综述（六）：总结

本文为大模型知识蒸馏原理综述第一篇，主要讲述当前大模型蒸馏相关的一些工作。






## 知识蒸馏简介


知识蒸馏，也被称为教师-学生神经网络学习算法，已经受到业界越来越多的关注。大型深度网络在实践中往往会获得良好的性能，因为当考虑新数据时，过度参数化会提高泛化性能。在知识蒸馏中，小网络（学生网络）通常是由一个大网络（教师网络）监督，算法的关键问题是如何将教师网络的知识传授给学生网络。通常把一个全新的更深的更窄结构的深度神经网络当作学生神经网络，然后把一个预先训练好的神经网络模型当作教师神经网络。

Hinton等人([@Distill])首先提出了教师神经网络-学生神经网络学习框架，通过最小化两个神经网络之间的差异来学习一个更窄更深的神经网络。记教师神经网络为
，它的参数为
，同时记学生神经网络为
，相应的参数为
。一般而言，学生神经网络相较于教师神经网络具有更少的参数。

文献([@Distill])提出的知识蒸馏（knowledge distillation，KD）方法，同时令学生神经网络的分类结果接近真实标签并且令学生神经网络的分类结果接近于教师神经网络的分类结果，即，

(10.3.5)
其中，
是交叉熵函数，
和
分别是学生网络和教师网络的输出，
是标签。公式 (10.3.5)中的第一项使得学生神经网络的分类结果接近预期的真实标签，而第二项的目的是提取教师神经网络中的有用信息并传递给学生神经网络，
是一个权值参数用来平衡两个目标函数。
是一个软化（soften）函数，将网络输出变得更加平滑。

公式 (10.3.5)仅仅从教师神经网络分类器输出的数据中提取有价值的信息，并没有从其他中间层去将教师神经网络的信息进行挖掘。因此，Romero等人[@FitNet]）进一步地开发了一种学习轻型学生神经网络的方法，该算法可以从教师神经网络中任意的一层来传递有用的信息给学生神经网络。此外，事实上，并不是所有的输入数据对卷积神经网络的计算和完成后续的任务都是有用的。例如，在一张包含一个动物的图像中，对分类和识别结果比较重要的是动物所在的区域，而不是那些无用的背景信息。所以，有选择性地从教师神经网络的特征图中提取信息是一个更高效的方式。于是，Zagoruyko和Komodakis（[@attentionTS]）提出了一种基于感知（Attention）损失函数的学习方法来提升学生神经网络的性能，该方法在学习学生神经网络的过程中，引入了感知模块（Attention），选择性地将教师神经网络中的信息传递给学生神经网络，并帮助其进行训练。感知图用来表达输入图像不同位置对最终分类结果的重要性。感知模块从教师网络生成感知图，并迁移到学生网络，如图


图10.3.3 一种基于感知（attention）的教师神经网络-学生神经网络学习算法

知识蒸馏是一种有效的帮助小网络优化的方法，能够进一步和剪枝、量化等其他压缩方法结合，训练得到精度高、计算量小的高效模型。



标准知识蒸馏

基于涌现能力的知识蒸馏
- 上下文学习
- 思维链
- 指令遵循



