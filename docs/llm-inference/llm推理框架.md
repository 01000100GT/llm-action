
## vLLM

适用于大批量Prompt输入，并对推理速度要求高的场景；





## Huggingface TGI


依赖HuggingFace模型，并且不需要为核心模型增加多个adapter的场景；





## DeepSpeed-MII

使用DeepSpeed库来部署LLM；





