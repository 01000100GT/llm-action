随着ChatGPT的迅速出圈，加速了大模型时代的变革。对于以Transformer、MOE结构为代表的大模型来说，传统的单机单卡训练模式肯定不能满足上千（万）亿级参数的模型训练，这时候我们就需要解决内存墙、通信墙、性能墙、调优墙等一系列问题，在单机多卡或者多机多卡进行模型训练。

最近，我也在探索大模型相关的一些技术，下面做一个简单的总结，后续争取每一个季度更新一次，目前最新更新为2023.07.03，本文主要涉及**AI集群、AI集群通信、大模型训练、大模型推理加速、大模型评估、大模型应用开发、AI编译器、大模型生态相关技术等**相关内容，同时，也对之前写过的一些大模型相关的文章进行了汇总，文档及配套代码均整理并放置在GitHub: [llm-action](https://github.com/liguodongiot/llm-action)，篇幅太长，建议先收藏后再阅读。

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/268e01d5834644f5bc503c665a876eb6~tplv-k3u1fbpfcp-watermark.image?)

另外，我创建了大模型学习交流群，供大家一起学习交流大模型相关的最新技术，目前已有5个群，可加我微信进群（加微信请备注来意，如：进大模型学习交流群+知乎）。一定要备注哟，否则不予通过。【点击】[加入大模型技术交流群](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247485828&idx=1&sn=7355c99bc907b972773f795cea9326c8&chksm=fd3be0d7ca4c69c10d842b0150a754178f9bd7691ec1e8a64c7a441822ca45833e718a9008bd&scene=21#wechat_redirect)。

## AI基础设施

### AI 集群-新增（2023.10.18）

作为大模型训练和推理的基础设施，AI 集群无疑是最重要的部分之一。国内头部大模型公司目前都是上千卡规模，而对于大模型不需要进行大规模预训练的中型公司，通常也有上百卡规模，用于满足线上或线下的模型训练和推理需求。

因此，做大模型时，需要找准自己的定位，当前的AI硬件资源、AI人力资源等。

如果做预训练，训练上百亿级规模大模型至少准备上百张高端AI显卡，具体可参考目前业界知名大模型模型预训练所耗的显卡和显存。

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6c9ac7dc96bf4ec188e7e2c7bc49f28b~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=706&h=380&s=60959&e=png&a=1&b=d3c2eb)

如果做模型全量训练，通常情况下显存的消耗除了模型权重本身，前向传播的激活值，反向传播的梯度，同时，还有优化器状态等等。根据ZeRO论文中所述需要16x模型参数量Byte。 对于一个 7.5B 的模型显存占用高达 120GB。如果显卡不够，考虑采用LoRA、Prefix Tuning等高效微调方法。

如果是模型推理，对于显存的消耗大约为1.2倍模型权重大小。

### AI处理器（加速卡）-新增（2023.06.30）

目前，主流的AI处理器无疑是NVIDIA的GPU，并且，英伟达针对不同的场景推出了不同的系列和型号。例如：L4用于AI视频，L40用于图像生成，H100系列则是大模型，GH200是图形推荐模型、矢量数据库和图神经网络。目前NVIDIA的GPU产品主要有GeForce、Tesla和Quadro三大系列，虽然，从硬件角度来看，它们都采用同样的架构设计，也都支持用作通用计算(GPGPU)，但因为它们分别面向的目标市场以及产品定位的不同，这三个系列的GPU在软硬件的设计和支持上都存在许多差异。其中，GeForce为消费级显卡，而Tesla和Quadro归类为专业级显卡。GeForce主要应用于游戏娱乐领域，而Quadro主要用于专业可视化设计和创作，Tesla更偏重于深度学习、人工智能和高性能计算。

* Tesla：A100（A800）、H100（H800）、A30、A40、V100、P100...
* GeForce：RTX 3090、RTX 4090 ...
* Quadro：RTX 6000、RTX 8000 ...

其中，A800/H800是针对中国特供版（低配版），相对于A100/H100，主要区别：

* A100的Nvlink最大总网络带宽为600GB/s，而A800的Nvlink最大总网络带宽为400GB/s。
* H100的Nvlink最大总网络带宽为900GB/s，而A800的Nvlink最大总网络带宽为400GB/s。

**其他的一些国外AI处理器**（加速卡）：

* AMD：GPU MI300X
* Intel：Xeon Phi、GPU、GNA
* Google：TPU（Tensor Processing Unit）

**国产AI处理器**（加速卡）：

* 华为昇腾NPU：昇腾910（用于训练和推理），昇腾310（用于推理）。采用自家设计的达芬奇架构。
* 海光DCU：8100系列（深算一号），以GPGPU架构为基础。
* 寒武纪：思元370、思元590。
* 百度-昆仑芯：采用的是其自研XPU架构。
* 阿里-平头哥：含光800。
* 壁仞：BR100系列通用GPU芯片（壁砺™100P产品形态为OAM模组、壁砺™104系列产品形态为PCIe板卡）。
* 燧原科技：云燧T1x/T2x训练系列、云燧i1x/i2x推理系列。采用其自研的 `GCU-CARA`架构。

除此之外，还有像摩尔线程、沐曦集成电路、天数智芯等发布的AI加速卡。随着美国对国内高端芯片的进一步封锁，希望国产芯片早日雄起。

### AI 集群网络通信-新增（2023.06.30）

#### 通信硬件

机器内通信：

* 共享内存，比如：CPU与CPU之间的通信可以通过共享内存。
* PCIe，通常是CPU与GPU之间的通信，也可以用于GPU与GPU之间的通信。
* NVLink（直连模式），通常是GPU与GPU之间的通信，也可以用于CPU与GPU之间的通信。

机器间通信：

* TCP/IP网络
* RDMA：远程直接内存访问，目前主要有如下三种技术：
  * InfiniBand
  * iWarp
  * RoCE v2

#### 通信软件

下面是一些常见的网络通信库：

* Gloo： Facebook 开源的一套集体通信库，提供了对机器学习中有用的一些集合通信算法。
* NCCL：英伟达基于 NVIDIA-GPU 的一套开源的集合通信库。
* OpenMPI：一个开源 MPI（消息传递接口 ）的实现，由学术，研究和行业合作伙伴联盟开发和维护。
* HCCL：华为开发的网络通信库。

#### 通信网络监控

* [nvbandwidth](https://github.com/NVIDIA/nvbandwidth)：用于测量 NVIDIA GPU 带宽的工具。
* [DCGM](https://github.com/NVIDIA/DCGM)：一个用于收集telemetry数据和测量 NVIDIA GPU 运行状况。

## 大模型算法

**模型结构**：

目前主流的大模型都是Transformer、MOE结构为基础进行构建，如果说Transformer结构使得模型突破到上亿参数量，MoE 稀疏混合专家结构使模型参数量产生进一步突破，达到数万亿规模。

**大模型算法**：

下图详细展示了AI大模型的发展历程：
![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/376fd2d6da6140288843ca5c528226d4~tplv-k3u1fbpfcp-watermark.image?)

可以说，Transformer 开创了继 MLP 、CNN和 RNN之后的第四大类模型。而基于Transformer结构的模型又可以分为Encoder-only、Decoder-only、Encoder-Decoder这三类。

* 仅编码器架构（Encoder-only）：自编码模型（破坏一个句子，然后让模型去预测或填补），更擅长理解类的任务，例如：文本分类、实体识别、关键信息抽取等。典型代表有：Bert、RoBERTa等。
* 仅解码器架构（Decoder-only）：自回归模型（将解码器自己当前步的输出加入下一步的输入，解码器融合所有已经输入的向量来输出下一个向量，所以越往后的输出考虑了更多输入），更擅长生成类的任务，例如：文本生成。典型代表有：GPT系列、LLaMA、OPT、Bloom等。
* 编码器-解码器架构（Encoder-Decoder）：序列到序列模型（编码器的输出作为解码器的输入），主要用于基于条件的生成任务，例如：翻译，概要等。典型代表有：T5、BART等。

### 大语言模型

**目前业界可以下载到的一些大语言模型**：

* ChatGLM-6B / ChatGLM2-6B ：清华开源的中英双语的对话语言模型。目前，第二代ChatGLM在官网允许的情况下可以进行商用。
* GLM-10B/130B ：双语（中文和英文）双向稠密模型。
* OPT-2.7B/13B/30B/66B ：Meta开源的预训练语言模型。
* LLaMA-7B/13B/30B/65B ：Meta开源的基础大语言模型。
* Alpaca（LLaMA-7B）：斯坦福提出的一个强大的可复现的指令跟随模型，种子任务都是英语，收集的数据也都是英文，因此训练出来的模型未对中文优化。
* BELLE（BLOOMZ-7B/LLaMA-7B/LLaMA-13B）：本项目基于 [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)，针对中文做了优化，模型调优仅使用由ChatGPT生产的数据（不包含任何其他数据）。
* Bloom-7B/13B/176B：可以处理46 种语言，包括法语、汉语、越南语、印度尼西亚语、加泰罗尼亚语、13 种印度语言（如印地语）和 20 种非洲语言。其中，Bloomz系列模型是基于 xP3 数据集微调。 推荐用于英语的提示（prompting）；Bloomz-mt系列模型是基于 xP3mt 数据集微调。推荐用于非英语的提示（prompting）。
* Vicuna(7B/13B)：由UC Berkeley、CMU、Stanford和 UC San Diego的研究人员创建的 Vicuna-13B，通过在 ShareGPT 收集的用户共享对话数据中微调 LLaMA 获得。其中，使用 GPT-4 进行评估，发现 Vicuna-13B 的性能在超过90%的情况下实现了与ChatGPT和Bard相匹敌的能力；同时，在 90% 情况下都优于 LLaMA 和 Alpaca 等其他模型。而训练 Vicuna-13B 的费用约为 300 美元。不仅如此，它还提供了一个用于训练、服务和评估基于大语言模型的聊天机器人的开放平台：[FastChat](https://github.com/lm-sys/FastChat)。
* [Baize](https://github.com/project-baize/baize-chatbot)：白泽是在LLaMA上训练的。目前包括四种英语模型：白泽-7B、13B 、 30B（通用对话模型）以及一个垂直领域的白泽-医疗模型，供研究 / 非商业用途使用，并计划在未来发布中文的白泽模型。白泽的数据处理、训练模型、Demo 等全部代码已经开源。
* [LLMZoo](https://github.com/FreedomIntelligence/LLMZoo)：来自香港中文大学和深圳市大数据研究院团队推出的一系列大模型，如：Phoenix（凤凰） 和 Chimera等。
* [MOSS](https://github.com/OpenLMLab/MOSS)：由复旦 NLP 团队推出的 MOSS 大语言模型。
* [baichuan-7B/13B、baichuan2-7B/13B](https://github.com/baichuan-inc/baichuan-7B)：由百川智能推出的大模型，可进行商用。
* [CPM-Bee](https://github.com/OpenBMB/CPM-Bee)：百亿参数的开源中英文双语基座大模型，可进行商用。
* 书生·浦语（InternLM 7B/20B）：来自上海人工智能实验室，开源可商用。
  ...

从上面可以看到，开源的大语言模型主要有三大类：GLM衍生的大模型（wenda、[ChatSQL](https://github.com/yysirs/ChatSQL)等）、LLaMA衍生的大模型（Alpaca、Vicuna、BELLE、Phoenix、Chimera等）、Bloom衍生的大模型（Bloomz、BELLE、Phoenix等）。

| 模型       | 训练数据量                                           | 模型参数   | 训练数据范围               | 词表大小 |
| ---------- | ---------------------------------------------------- | ---------- | -------------------------- | -------- |
| LLaMA      | 1T～1.4T tokens(其中，7B/13B使用1T，33B/65B使用1.4T) | 7B～65B    | 以英语为主要语言的拉丁语系 | 32000    |
| ChatGLM-6B | 约 1T tokens                                         | 6B         | 中文、英语                 | 130528   |
| Bloom      | 1.6TB预处理文本，转换为 350B 唯一 tokens             | 300M\~176B | 46种自然语言，13种编程语言 | 250680   |

从表格中可以看到，对于像ChatGLM-6B、LLaMA、Bloom这类大模型，要保证基座模型有比较好的效果，至少需要保证上千亿、万亿级的Token量。

目前来看，LLaMA无疑是其中最闪亮的星。但是国内关于LLaMA比较大的一个争论就是LLaMA是以英语为主要语言的拉丁语系上进行训练的，LLaMA词表中的中文token比较少（只有几百个），需不需要扩充词表？如果不扩充词表，中文效果会不会比较差？

* 如果不扩充词表，对于中文效果怎么样？根据Vicuna官方的报告，经过Instruction Turing的Vicuna-13B已经有非常好的中文能力。
* LLaMA需不需要扩充词表？根据[Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)和[BELLE](https://github.com/LianjiaTech/BELLE)的报告，扩充中文词表，可以提升中文编解码效率以及模型的性能。但是扩词表，相当于从头初始化开始训练这些参数。如果想达到比较好的性能，需要比较大的算力和数据量。同时，Chinese-LLaMA-Alpaca也指出在进行第一阶段预训练（冻结transformer参数，仅训练embedding，在尽量不干扰原模型的情况下适配新增的中文词向量）时，模型收敛速度较慢。如果不是有特别充裕的时间和计算资源，建议跳过该阶段。因此，虽然扩词表看起来很诱人，但是实际操作起来，还是很有难度的。

下面是BELLE针对是否扩充词表，数据质量、数据语言分布、数据规模对于模型性能的对比：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cd50114de9aa48b2b6404531d477bf6b~tplv-k3u1fbpfcp-watermark.image?)

其中，**BELLE-0.5M-CLEAN**是从230万指令数据中清洗得到0.5M数据（包含单轮和多轮对话数据）。LLaMA-7B-EXT是针对LLaMA做了中文词表扩充的预训练模型。

下面是Chinese-LLaMA-Alpaca针对中文Alpaca-13B、中文Alpaca-Plus-7B、中文Alpaca-Plus-13B的效果对比：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/61f342d4f77a4486a9fd94aaaef677c8~tplv-k3u1fbpfcp-watermark.image?)

其中，Plus系列Alpaca是在原版LLaMA的基础上扩充了中文词表，使用了120G中文通用纯文本数据进行二次预训练。

因此，如果既想要中文词表，又没有很大的算力，那建议直接使用ChatGLM-6B或者使用BELLE和Chinese-LLaMA-Alpaca进行中文词表扩充后训练好的模型作为Base模型。

### 多模态大模型

**目前业界可以下载到的一些多模态大模型**：

* [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)：沙特阿拉伯阿卜杜拉国王科技大学的研究团队开源。
* [LLaVA](https://github.com/haotian-liu/LLaVA)：由威斯康星大学麦迪逊分校，微软研究院和哥伦比亚大学共同出品。
* [VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B/)：开源的，支持**图像、中文和英文**的多模态对话语言模型，语言模型基于 [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)，具有 62 亿参数；图像部分通过训练 [BLIP2-Qformer](https://arxiv.org/abs/2301.12597) 构建起视觉模型与语言模型的桥梁，整体模型共78亿参数。
* [VisCPM](https://github.com/OpenBMB/VisCPM)：于CPM基础模型的中英双语多模态大模型。

### 领域大模型-新增（2023.06.30）

#### 金融领域大模型

* [轩辕](https://github.com/Duxiaoman-DI/XuanYuan)：在BLOOM-176B的基础上针对中文通用领域和金融领域进行了针对性的预训练与微调。

#### 法律领域大模型

* [ChatLaw](https://github.com/PKU-YuanGroup/ChatLaw)：由北京大学开源的大模型，主要有13B、33B。
* [LexiLaw](https://github.com/CSHaitao/LexiLaw)：LexiLaw 是一个经过微调的中文法律大模型，它基于 ChatGLM-6B 架构，通过在法律领域的数据集上进行微调，使其在提供法律咨询和支持方面具备更高的性能和专业性。

## 大模型训练

### 分布式并行及显存优化技术

**并行技术**：

* 数据并行（如：PyTorch DDP）
* 张量并行（如：Megatron-LM（1D）、Colossal-AI（2D、2.5D、3D））
* 流水线并行（如：GPipe、PipeDream、PipeDream-2BW、PipeDream Flush（1F1B））
* 多维混合并行（如：3D并行（数据并行、模型并行、流水线并行））
* 自动并行（如：Alpa（自动算子内/算子间并行））
* 优化器相关的并行（如：ZeRO（**零冗余优化器**，在执行的逻辑上是数据并行，但可以达到模型并行的显存优化效果）、PyTorch FSDP）

- MOE并行
- 序列并行

具体可查看我写的分布式并行技术文章：https://github.com/liguodongiot/llm-action/tree/main#llm%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%A1%8C%E6%8A%80%E6%9C%AF

**显存优化技术**：

* 重计算(Recomputation)：Activation checkpointing(Gradient checkpointing)，本质上是一种用时间换空间的策略。
* 卸载（Offload）技术：一种用通信换显存的方法，简单来说就是让模型参数、激活值等在CPU内存和GPU显存之间左右横跳。如：ZeRO-Offload、ZeRO-Infinity等。
* 混合精度（BF16/FP16）：降低训练显存的消耗，还能将训练速度提升2-4倍。
  * BF16 计算时可避免计算溢出，出现Inf case。
  * FP16 在输入数据超过65506 时，计算结果溢出，出现Inf case。

### 分布式训练框架

**如何选择一款分布式训练框架**？

* **训练成本**：不同的训练工具，训练同样的大模型，成本是不一样的。对于大模型，训练一次动辄上百万/千万美元的费用。合适的成本始终是正确的选择。
* **训练类型**：是否支持数据并行、张量并行、流水线并行、多维混合并行、自动并行等
* **效率**：将普通模型训练代码变为分布式训练所需编写代码的行数，我们希望越少越好。
* **灵活性**：你选择的框架是否可以跨不同平台使用？

**常见的分布式训练框架**：

* 第一类：深度学习框架自带的分布式训练功能。如：TensorFlow、PyTorch、MindSpore、Oneflow、PaddlePaddle等。
* 第二类：基于现有的深度学习框架（如：PyTorch、Flax）进行扩展和优化，从而进行分布式训练。如：Megatron-LM（张量并行）、DeepSpeed（Zero-DP）、Colossal-AI（高维模型并行，如2D、2.5D、3D）、Alpa（自动并行）等

**目前训练超大规模语言模型主要有两条技术路线**：

1. TPU + XLA + TensorFlow/JAX ：由Google主导，由于TPU和自家云平台GCP深度绑定。
2. GPU + PyTorch + Megatron-LM + DeepSpeed ：由NVIDIA、Meta、MicroSoft大厂加持，社区氛围活跃，也更受到大家欢迎。

对于国内来说，华为昇腾在打造 AI 全栈软硬件平台（昇腾NPU+CANN+MindSpore+MindFormers）。不过目前整个生态相对前两者，还差很远。

### 参数高效微调（PEFT）技术

在面对特定的下游任务时，如果进行Full FineTuning（即对预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。

PEFT技术旨在**通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本**。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。因此，PEFT技术可以在提高模型效果的同时，大大缩短模型训练时间和计算成本，让更多人能够参与到深度学习研究中来。除此之外，FEFT可以缓解全量微调带来灾难性遗忘的问题。

* **Prefix Tuning**：与full fine-tuning更新所有参数的方式不同，该方法是在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而Transformer中的其他部分参数固定。该方法其实和构造Prompt类似，只是Prompt是人为构造的“显式”的提示,并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。
  同时，为了防止直接更新Prefix的参数导致训练不稳定的情况，他们在Prefix层前面加了MLP结构(相当于将Prefix分解为更小维度的Input与MLP的组合后输出的结果)，训练完成后，只保留Prefix的参数。
* **Prompt Tuning**：该方法可以看作是Prefix Tuning的简化版本，只在输入层加入prompt tokens，并不需要加入MLP进行调整来解决难训练的问题。随着预训练模型参数量的增加，Prompt Tuning的方法会逼近fine-tuning的结果。
* **P-Tuning**：该方法的提出主要是为了解决这样一个问题：大模型的Prompt构造方式严重影响下游任务的效果。P-Tuning将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对prompt embedding进行一层处理。
* **P-Tuning v2**：让Prompt Tuning能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌Fine-tuning的结果。相比Prompt Tuning和P-tuning的方法，P-Tuning v2方法在多层加入了Prompts tokens作为输入，带来两个方面的好处：
  1. 带来更多可学习的参数（从P-tuning和Prompt Tuning的0.1%增加到0.1%-3%），同时也足够参数高效。
  2. 加入到更深层结构中的Prompt能给模型预测带来更直接的影响。
* **Adapter Tuning**：该方法设计了Adapter结构（首先是一个down-project层将高维度特征映射到低维特征，然后过一个非线形层之后，再用一个up-project结构将低维特征映射回原来的高维特征；同时也设计了skip-connection结构，确保了在最差的情况下能够退化为identity），并将其嵌入Transformer的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的Adapter结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数）。
* **LoRA**：在涉及到矩阵相乘的模块，引入A、B这样两个低秩矩阵模块去模拟full fine-tuning的过程，相当于只对语言模型中起关键作用的低秩本质维度进行更新。
* QLoRA：使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。目前，训练速度较慢。
* **AdaLoRA**：对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。
  ...

**具体可查看我写的大模型微调技术文章：https://github.com/liguodongiot/llm-action#llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86**

**典型应用**：

1. [ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) ：一种平价的chatgpt实现方案，基于清华的 [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) + LoRA 进行finetune。
2. [Alpaca-Lora](https://github.com/tloen/alpaca-lora)：使用低秩自适应（LoRA）复现斯坦福羊驼的结果。Stanford Alpaca 是在 LLaMA 整个模型上微调，而 Alpaca-Lora 则是利用 Lora 技术，在冻结原模型 LLaMA 参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅微调的成本显著下降，还能获得和全模型微调类似的效果。
3. [BLOOM-LORA](https://github.com/linhduongtuan/BLOOM-LORA)：由于LLaMA的限制，我们尝试使用Alpaca-Lora重新实现BLOOM-LoRA。

**PEFT实现**：

1. [PEFT](https://github.com/huggingface/peft)：Huggingface推出的PEFT库。
2. [unify-parameter-efficient-tuning](https://github.com/jxhe/unify-parameter-efficient-tuning)：一个参数高效迁移学习的统一框架。

**高效微调技术优缺点**：

相比全参数微调，大部分的高效微调技术可以显著降低对于显存的需求，同时，可以减轻灾难性遗忘。

但是大部分的高效微调技术目前存在的两个问题：

1. 推理速度会变慢
2. 模型精度会变差

### RLHF(人工反馈强化学习)-新增（2023.07.03）

根据 OpenAI 之前做的一些实验，可以看到使用了 PPO（近端策略优化）算法的 RLHF 模型整体上都更好一些。当把结果提供给人类时，相比于 SFT 模型和通过 prompt 化身为助理的基础模型，人类也基本更喜欢来自 RLHF 模型的 token。

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9c370e65027b403798ddf46bc8d83d6d~tplv-k3u1fbpfcp-watermark.image?)

那 RLHF 为什么能让模型更好呢？目前 AI 研究界还没有找到一个得到大家认可的理论，一种可能**与比较和生成的计算难度之间的不对称性有关**。

举个例子说明一下：假设我们要让一个模型写一首关于回形针的俳句。如果你是一位正努力创建训练数据的合同工，正在为 SFT 模型收集数据。那么你该怎样写出一首关于回形针的好俳句呢？而你可能并不是一位优秀的俳句诗人。但是，如果给你几首俳句，你却有能力辨别它们中哪首更好一些。也就是说，比起创建一个好样本，判断哪个样本更好是简单得多的任务。因此，这种不对称性可能使得比较是一种更好的方法，能更好地利用人类的判断来创造出好一些的模型。

但目前来看，RLHF 并不总是会为基石模型带来提升。在某些情况下，RLHF 模型会失去一些熵，也就是说它们会输出更加单调、变化更少的结果。而基础模型的熵更高，可以输出更加多样化的结果。

#### RLHF开源工具

下面是目前开源的一些RLHF工具：

* [DeepSpeed Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)：基于Opt、LLaMA、Bloom系列模型进行示例。
* [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)：基于LLaMA系列模型进行示例。

### 影响大模型性能的主要因素

OpenAI的论文**Scaling Laws for Neural Language Models**中列举了影响模型性能最大的三个因素：**计算量**、**数据集大小**、**模型参数量**。也就是说，当其他因素不成为瓶颈时，计算量、数据集大小、模型参数量这3个因素中的单个因素指数增加时，loss会线性的下降。

除了以上的因素之外，还有一个比较大的影响因素就是**数据质量**。在微软的论文**Instruction Tuning with GPT-4**中指出，同样基于LLaMA模型，使用GPT3和GPT4产生的数据，对模型进行Instruction Turing，可以看到GPT4的数据微调过的模型效果远远好于GPT3数据微调的模型，可见数据质量带来的影响。同样的，Vicuna（7B/13B）的Instruction Turing中，也对shareGPT的数据做了很细致的清洗工作。

## 大模型评估

### 衡量大模型水平

要评估一个大型语言模型的水平，可以从以下几个维度提出具有代表性的问题。

* **理解能力**：提出一些需要深入理解文本的问题，看模型是否能准确回答。
* **语言生成能力**：让模型生成一段有关特定主题的文章或故事，评估其生成的文本在结构、逻辑和语法等方面的质量。
* **知识面广度**：请模型回答关于不同主题的问题，以测试其对不同领域的知识掌握程度。这可以是关于科学、历史、文学、体育或其他领域的问题。一个优秀的大语言模型应该可以回答各种领域的问题，并且准确性和深度都很高。
* **适应性**：让模型处理各种不同类型的任务，例如：写作、翻译、编程等，看它是否能灵活应对。
* **长文本理解**：提出一些需要处理长文本的问题，例如：提供一篇文章，让模型总结出文章的要点，或者请模型创作一个故事或一篇文章，让其有一个完整的情节，并且不要出现明显的逻辑矛盾或故事结构上的错误。一个好的大语言模型应该能够以一个连贯的方式讲述一个故事，让读者沉浸其中。
* **长文本生成**：请模型创作一个故事或一篇文章，让其有一个完整的情节，并且不要出现明显的逻辑矛盾或故事结构上的错误。一个好的大语言模型应该能够以一个连贯的方式讲述一个故事，让读者沉浸其中。
* **多样性**：提出一个问题，让模型给出多个不同的答案或解决方案，测试模型的创造力和多样性。
* **情感分析和推断**：提供一段对话或文本，让模型分析其中的情感和态度，或者推断角色间的关系。
* **情感表达**：请模型生成带有情感色彩的文本，如描述某个场景或事件的情感、描述一个人物的情感状态等。一个优秀的大语言模型应该能够准确地捕捉情感，将其表达出来。
* **逻辑推理能力**：请模型回答需要进行推理或逻辑分析的问题，如概率或逻辑推理等。这可以帮助判断模型对推理和逻辑思考的能力，以及其在处理逻辑问题方面的准确性。例如：“所有的动物都会呼吸。狗是一种动物。那么狗会呼吸吗？”
* **问题解决能力**：提出实际问题，例如：数学题、编程问题等，看模型是否能给出正确的解答。
* **道德和伦理**：测试模型在处理有关道德和伦理问题时的表现，例如：“在什么情况下撒谎是可以接受的？”
* **对话和聊天**：请模型进行对话，以测试其对自然语言处理的掌握程度和能力。一个优秀的大语言模型应该能够准确地回答问题，并且能够理解人类的语言表达方式。

### **LLM评估面临的挑战**

- 无法获取模型的训练数据分布信息
- 生成式输出难以进行定量评估
- 通用模型不同领域的表现参差不齐
- 单一指标难以概括模型的多样能力

### **大模型评估方法**

**人工评估**：LIMA、Phoenix

**使用 GPT-4 的反馈进行自动评估**：Vicuna、Phoenix、Chimera、BELLE

**指标评估**（BLEU-4、ROUGE分数）：ChatGLM-6B；对于像ROUGE-L分数的指标评估，有些地方称其为非自然指令评估（Unnatural Instruction Evaluation）。

**Chatbot Arena**：目前用来衡量一个模型好不好的东西基本都是基于一些学术的benchmark，比如在一个某个NLP任务上构建一个测试数据集，然后看测试数据集上准确率多少。然而，这些学术benchmark（如HELM）在大模型和聊天机器人上就不好用了。其原因在于：

* 由于评判聊天机器人聊得好不好这件事是非常主观的，因此，现有的方法很难对其进行衡量。
* 这些大模型在训练的时候就几乎把整个互联网的数据都扫了一个遍，因此，很难保证测试用的数据集没有被看到过。甚至更进一步，用测试集直接对模型进行「特训」，如此一来表现必然更好。
* 理论上我们可以和聊天机器人聊任何事情，但很多话题或者任务在现存的benchmark里面根本就不存在。

因此，Chatbot Arena 的做法是放弃benchmark，通过对抗，实时聊天，两两比对人工进行打分，采用elo分数进行评测。

**大模型评估工具**：

* [OpenAI evals](https://github.com/openai/evals)：OpenAI的自动化评估脚本，核心思路就是通过写prompt模版来自动化评估。
* [PandaLM](https://github.com/WeOpenML/PandaLM)：其是直接训练了一个自动化打分模型，0,1,2三分制用模型对两个候选模型进行打分。

**如何构建评估数据集**

- 采用渐进方式构建，从一开始就收集各类样本
- 利用语言模型自动生成更多测试用例
- 持续从用户反馈中获取新样本,发现瑕疵
- 考虑提高测试覆盖率，覆盖真实用例范围

## 大模型推理

### 大模型推理加速

模型推理作为模型投产的最后一公里，需要确保模型精度的同时追求极致的推理性能。相比传统模型来说，大模型面临着更多的挑战。

当前优化模型最主要技术手段概括来说有以下三个层面：

* 算法层面：蒸馏、量化
* 软件层面：计算图优化、模型编译
* 硬件层面：FP8（NVIDIA H系列GPU开始支持FP8，兼有fp16的稳定性和int8的速度）

**推理加速框架**：

* **FasterTransformer**：英伟达推出的FasterTransformer不修改模型架构而是在计算加速层面优化 Transformer 的 encoder 和 decoder 模块。具体包括如下：
  * 尽可能多地融合除了 GEMM 以外的操作
  * 支持 FP16、INT8、FP8
  * 移除 encoder 输入中无用的 padding 来减少计算开销
* **TurboTransformers**：腾讯推出的 TurboTransformers 由 computation runtime 及 serving framework 组成。加速推理框架适用于 CPU 和 GPU，最重要的是，它可以无需预处理便可处理变长的输入序列。具体包括如下：
  * 与 FasterTransformer 类似，它融合了除 GEMM 之外的操作以减少计算量
  * smart batching，对于一个 batch 内不同长度的序列，它也最小化了 zero-padding 开销
  * 对 LayerNorm 和 Softmax 进行批处理，使它们更适合并行计算
  * 引入了模型感知分配器，以确保在可变长度请求服务期间内存占用较小


## 大模型压缩（2023.10.24）


### 大模型量化

根据应用量化压缩模型的阶段，可以将大模型量化分为：量化感知训练、量化感知微调、训练后量化。

目前业界一些量化方案如下：

训练后量化：

- SmoothQuant
- ZeroQuant
- GPTQ
- LLM.int8()
- ...

量化感知训练：

- [大模型量化感知训练开山之作：LLM-QAT](https://zhuanlan.zhihu.com/p/647589650)

量化感知微调：

- QLoRA
- PEQA


### 大模型蒸馏

大模型蒸馏就是利用LLM作为教师的蒸馏方法。根据这些方法是否将LLM的涌现能力（EA）提炼成小语言模型（SLM）来对这些方法进行分类。 因此，我们将这些方法分为两个不同的类别：标准 KD 和基于 EA 的 KD。

目前业界一些蒸馏方案如下：

**Standard KD**:

使学生模型学习教师模型(LLM)所拥有的常见知识，如输出分布和特征信息，这种方法类似于传统的KD。

- MINILLM
- GKD

**EA-based KD**:

不仅仅是将LLM的常见知识转移到学生模型中，还涵盖了蒸馏它们独特的涌现能力。具体来说，EA-based KD又分为了上下文学习（ICL）、思维链（CoT）和指令跟随（IF）。

In-Context Learning：

- In-Context Learning distillation

Chain-of-Thought：

- MT-COT
- Fine-tune-CoT
- DISCO
- SCOTT
- SOCRATIC CoT

Instruction Following：

- Lion


## 大模型生态相关技术-新增（2023.06.30）

大模型是基座，要想让其变成一款产品，我们还需要一些其他相关技术：

### LLM 应用开发工具

* langchain：一个用于构建基于大型语言模型（LLM）的应用程序的库。它可以帮助开发者将LLM 与其他计算或知识源结合起来，创建更强大的应用程序。
* llama-index：一个将大语言模型和外部数据连接在一起的工具。
* gpt-cache：LLM 语义缓存层（caching layer），它采用语义缓存（semantic cache）技术，能够存储 LLM 响应，从而显著减少检索数据所需的时间、降低 API 调用开销、提升应用可扩展性。

### 向量数据库

* Pinecone
* Milvus
* Vespa
* Weaviate

总的来说，如果想快速验证，Pinecone 是个不错的选择。如果想拥有更灵活的查询方式，可以考虑 Vespa 或 Weaviate.如果需要更好的可扩展性和可靠性，那么经过大客户验证的 Vespa 或 Milvus 可能是不错的选择。

## 经验与教训

**经验**：

* 对于同一模型，选择不同的训练框架，对于资源的消耗情况可能存在显著差异（比如使用Huggingface Transformers和DeepSpeed训练OPT-30相对于使用Alpa对于资源的消耗会低不少）。
* 进行大模型模型训练时，先使用小规模模型（如：OPT-125m/2.7b）进行尝试，然后再进行大规模模型（如：OPT-13b/30b...）的尝试，便于出现问题时进行排查。目前来看，业界也是基于相对较小规模参数的模型（6B/7B/13B）进行的优化，同时，13B模型经过指令精调之后的模型效果已经能够到达GPT4的90%的效果。
* 对于一些国产AI加速卡，目前来说，坑还比较多，如果时间不是时间非常充裕，还是尽量选择Nvidia的AI加速卡。
* 目前业界的很多大模型训练后量化方法都只是降低了显存的消耗，对于大模型推理速度不仅没有提升反而会下降。目前来说，比较好的方法有SmoothQuant、AWQ。

**教训**：

* 针对已有的环境进行分布式训练环境搭建时，一定要注意之前环境的python、pip、virtualenv、setuptools的版本。不然创建的虚拟环境即使指定对了Python版本，也可能会遇到很多安装依赖库的问题（GPU服务器能够访问外网的情况下，建议使用Docker相对来说更方便）。
* 遇到需要升级GLIBC等底层库需要升级的提示时，一定要慎重，不要轻易升级，否则，可能会造成系统宕机或很多命令无法操作等情况。



## 结语

实践出真知，以上是这段时间进行大模型实践的一点点总结，写的有一些主观和片面，后续会持续更新自己研究大模型获得的一些认知和实践经验，希望能够帮助大家，欢迎点赞收藏加关注。
