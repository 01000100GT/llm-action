


低秩适应方法(LoRA)可以在很大程度上减少训练参数数量,以微调大型语言模型(LLM),然而,仍需要昂贵的激活记忆更新低秩权重。减少LoRA层数或使用激活重计算可能会损害微调性能或增加计算开销。


在本文中,我们提出了LoRA-FA,一种高效的微调方法,可以减少激活记忆的需求,而无需性能下降和昂贵的重计算。LoRA-FA选择冻结A的投影下权重和更新B的投影上权重在每个LoRA层中。

它确保在LLM微调期间,模型权重的变化存在于低秩空间中,同时消除存储全秩输入激活的要求。我们跨越多个模型类型(RoBERTa、T5、LLaMA)和模型大小进行广泛的实验。

我们的结果显示,LoRA-FA可以在不同任务中比全参数微调和LoRA更准确地微调,同时LoRA-FA相对于LoRA可以减少总体内存成本高达1.4×。


----


本文提出了一种名为LoRA-FA的内存高效的细调方法，通过冻结A的投影下权重并仅更新B的投影上权重，将模型权重的变化限制在低秩空间中，从而减少了激活内存的需求。



方法的详细步骤：

 (1). 设计LoRA-FA方法：LoRA-FA方法通过冻结A的投影下权重并仅更新B的投影上权重，将模型权重的变化限制在低秩空间中，从而减少了激活内存的需求。 
 (2). LoRA-FA的集成：LoRA-FA可以与其他内存优化技术相结合，提高其利用率。 
 (3). LoRA-FA与梯度压缩的关系：LoRA方法通过更新A和B两个低秩矩阵，并使用AB作为预训练和冻结权重W的变化，即W + α∆W = W + αAB。

 LoRA-FA方法冻结了W和A，并仅在细调过程中更新B。在模型适应过程中，权重的变化将被限制在低秩空间中。

 (4). 低秩模型适应：在细调过程中，冻结初始化的A和预训练的W，并更新投影上权重B。
 因此，权重的变化将限制在由A的列空间定义的低秩空间中。 

 (5). 内存复杂度：对LoRA-FA的内存复杂度进行详细研究。LoRA-FA模块仅计算B的梯度，其具有d_out × r个元素。在GPT类型的模型中，总的可训练参数是n_r/2，即LoRA中可训练参数数量的一半。因此，在16位混合精度训练中，模型权重和适配器相关状态的内存成本为2n + 8n_r字节。相比于全参数细调，LoRA-FA通过显著减少可训练参数和输入激活的数量，具有内存效率。LoRA-FA可以与先进的内存优化方法相结合，如权重量化、权重分片和选择性激活重计算。 

 (6). 权重量化：LoRA-FA可以将模型权重量化为较低的位宽，以减少模型权重的内存开销，而不影响细调性能。 

 (7). 权重分片：在使用数据并行ism的多个GPU上训练LLM时，可以将权重分片或使用ZeRO stage-3技术与LoRA-FA相结合，将模型权重分片到不同的GPU上，从而降低每个GPU的内存开销。 

 (8). 选择性激活重计算：可以使用选择性激活重计算来重新计算部分模型组件的输入，以减少激活内存开销。通过选择性激活重计算，可以在不需要存储LoRA层输入的情况下平衡激活成本和重计算成本。







