



## colossalai

- https://colossalai.org/zh-Hans/docs/advanced_tutorials/integrate_mixture_of_experts_into_your_model/

## paddle

- https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/06_distributed_training/moe_cn.html


```
python -m paddle.distributed.launch --gpus=0,1,2,3,4,5,6,7 --log_dir logs train_moe.py
```

## deepspeed

- https://www.deepspeed.ai/tutorials/mixture-of-experts/




- https://www.deepspeed.ai/tutorials/mixture-of-experts/
- https://github.com/microsoft/DeepSpeedExamples/blob/master/training/cifar/run_ds_moe.sh
- https://colossalai.org/zh-Hans/docs/advanced_tutorials/integrate_mixture_of_experts_into_your_model/




