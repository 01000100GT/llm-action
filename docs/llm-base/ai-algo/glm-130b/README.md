


- 预训练语言模型：GLM: https://zhuanlan.zhihu.com/p/641499380




GLM-130B 对超过 4000 亿个双语标记（2000 亿英文和 2000 亿中文标记）进行了预训练。

它的预训练目标由两部分组成：

第一部分（95%）是自监督的预训练，即在公开的大规模语料库以及其他一些较小的中文语料库上的自回归空白填充。

第二部分（5%）是在 T0++18 和 DeepStruct19 中 70 个不同数据集的抽样子集上进行多任务指令预训练，格式为基于指令的多任务多提示序列到序列的生成。

这种设计使 GLM-130B 可以在其他数据集上进行了零样本学习，以及从英文到中文的零样本迁移。



