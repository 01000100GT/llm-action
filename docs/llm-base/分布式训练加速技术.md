




分布式并行技术：
	- 数据并行
	- 张量并行
	- 流水线并行
	- MOE并行（稀疏化）
	- ZeRO
	- 序列并行（LayerNorm 和 Dropout 的计算被平摊到了各个设备上，减少了计算资源的浪费；LayerNorm 和 Dropout 所产生的激活值也被平摊到了各个设备上，进一步降低了显存开销。）

混合精度训练：
	- FP16 / BF16
	- FP8（NVIDIA H系列GPU开始支持FP8，兼有FP16的稳定性和INT8的速度），Nvidia Transformer Engine 兼容 FP8 框架，主要利用这种精度进行 GEMM（通用矩阵乘法）计算，同时以 FP16 或 FP32 高精度保持主权重和梯度。  MS-AMP训练框架 (使用FP8进行训练)，与广泛采用的 BF16 混合精度方法相比，内存占用减少 27% 至 42%，权重梯度通信开销显著降低 63% 至 65%。运行速度比广泛采用的 BF16 框架（例如 Megatron-LM）快了 64%，比 Nvidia Transformer Engine 的速度快了 17%。


重计算(Recomputation)/梯度检查点(gradient checkpointing)：一种在神经网络训练过程中使动态计算只存储最小层数的技术。一种用计算换显存的方法。通过减少保存的激活值来压缩模型占用空间，在计算梯度时必须重新计算没有存储的激活值。

梯度累积：它将多个Batch训练数据的梯度进行累积，在达到指定累积次数后，使用累积梯度统一更新一次模型参数，以达到一个较大Batch Size的模型训练效果。累积梯度等于多个Batch训练数据的梯度的平均值。

FlashAttention v1 and v2 ：通过分块计算和kernel融合，减少了HBM访问次数，实现了计算加速，同时减少了显存占用。  参考：Megatron-deepspeed ， 训练推理都可以使用。

MQA / GQA： 一定程度的Key value的共享，从而可以使模型体积减小，减少了数据的读取。


集群网络通信硬件优化：
	- 服务器内：nvlink > pcie 
	- 服务器间：nvlink > InfiniBand （256-nvlink）> tcp/ip

更高性能的AI芯片：
	- H200>H100>A100





卸载（Offload）技术：一种用通信换显存的方法，简单来说就是让模型参数、激活值等在CPU内存和GPU显存之间左右横跳。如：ZeRO-Offload、ZeRO-Infinity等。
混合精度（BF16/FP16）：降低训练显存的消耗，还能将训练速度提升2-4倍。

