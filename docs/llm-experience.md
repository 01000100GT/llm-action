





微调：


PEFT总是有局限性，基于低秩的微调可能并不always work，比如：finetune与pretrain的gap过大的时候，比如中英差异。


微调的过程不是让模型适应另外的数据分布，而是让模型更好的激发出本身的表征能力。






量化：








