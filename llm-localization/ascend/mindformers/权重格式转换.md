


## 模型格式权重转换


- [离线权重转换](https://gitee.com/mindspore/mindformers/blob/r1.0/docs/feature_cards/Transform_Ckpt.md)

- https://gitee.com/mindspore/mindformers/blob/r1.0/mindformers/tools/transform_ckpt.py
- https://gitee.com/mindspore/mindspore/blob/v2.2.14/mindspore/python/mindspore/parallel/checkpoint_transform.py
- https://www.mindspore.cn/tutorials/experts/zh-CN/master/parallel/model_transformation.html



ConvertWeight支持对torch权重和mindspore权重的格式互转

- https://gitee.com/mindspore/mindformers/blob/dev/docs/feature_cards/Convert_Weight.md

- https://gitee.com/mindspore/mindformers/blob/dev/research/qwen/convert_weight.py
- https://gitee.com/mindspore/mindformers/blob/dev/research/qwen/convert_reversed.py

- https://gitee.com/mindspore/mindformers/blob/dev/mindformers/utils/convert_utils.py

- 统一转换入口：https://gitee.com/mindspore/mindformers/blob/dev/convert_weight.py



### 打印pytorch格式模型权重


```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/workspace/Qwen-7B-Chat", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("/workspace/Qwen-7B-Chat", device_map="cpu", torch_dtype=torch.float16, trust_remote_code=True)
print(model)

for name, param in model.named_parameters():
    print(name)


QWenLMHeadModel(
  (transformer): QWenModel(
    (wte): Embedding(151936, 4096)
    (drop): Dropout(p=0.0, inplace=False)
    (rotary_emb): RotaryEmbedding()
    (h): ModuleList(
      (0-31): 32 x QWenBlock(
        (ln_1): RMSNorm()
        (attn): QWenAttention(
          (c_attn): Linear(in_features=4096, out_features=12288, bias=True)
          (c_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (attn_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): RMSNorm()
        (mlp): QWenMLP(
          (w1): Linear(in_features=4096, out_features=11008, bias=False)
          (w2): Linear(in_features=4096, out_features=11008, bias=False)
          (c_proj): Linear(in_features=11008, out_features=4096, bias=False)
        )
      )
    )
    (ln_f): RMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)
)


transformer.wte.weight
transformer.h.0.ln_1.weight
transformer.h.0.attn.c_attn.weight
transformer.h.0.attn.c_attn.bias
transformer.h.0.attn.c_proj.weight
transformer.h.0.ln_2.weight
transformer.h.0.mlp.w1.weight
transformer.h.0.mlp.w2.weight
transformer.h.0.mlp.c_proj.weight
...
transformer.h.31.ln_1.weight
transformer.h.31.attn.c_attn.weight
transformer.h.31.attn.c_attn.bias
transformer.h.31.attn.c_proj.weight
transformer.h.31.ln_2.weight
transformer.h.31.mlp.w1.weight
transformer.h.31.mlp.w2.weight
transformer.h.31.mlp.c_proj.weight
transformer.ln_f.weight
lm_head.weight


```
	

## 打印mindspore格式模型权重

```	
from mindformers.tools.register.config import MindFormerConfig
from mindformers.tools.utils import str2bool
from qwen_config import QwenConfig
from qwen_model import QwenForCausalLM
from qwen_tokenizer import QwenTokenizer

yaml_path = "/workspace/mindformers/research/qwen/run_qwen_7b_infer.yaml"
config = MindFormerConfig(yaml_path)
model_config = QwenConfig.from_pretrained(yaml_path)
model = QwenForCausalLM(model_config)
print(model)


QwenForCausalLM<
  (transformer): QwenModel<
    (wte): LlamaEmbedding<>
    (drop): Dropout<p=0.0>
    (layers): CellList<
      (0): QwenDecodeLayer<
        (attention_norm): LlamaRMSNorm<>
        (ffn_norm): LlamaRMSNorm<>
        (attention): LLamaAttention<
          (apply_rotary_emb): LlamaRotaryEmbedding<>
          (wq): Linear<>
          (wk): Linear<>
          (wv): Linear<>
          (wo): Linear<>
          (kvcache_mgr): KVCacheMgr<>
          >
        (feed_forward): QwenFeedForward<
          (silu): LlamaSiLU<>
          (w1): Linear<>
          (w2): Linear<>
          (w3): Linear<>
          >
        >
      ...
      (31): QwenDecodeLayer<
        (attention_norm): LlamaRMSNorm<>
        (ffn_norm): LlamaRMSNorm<>
        (attention): LLamaAttention<
          (apply_rotary_emb): LlamaRotaryEmbedding<>
          (wq): Linear<>
          (wk): Linear<>
          (wv): Linear<>
          (wo): Linear<>
          (kvcache_mgr): KVCacheMgr<>
          >
        (feed_forward): QwenFeedForward<
          (silu): LlamaSiLU<>
          (w1): Linear<>
          (w2): Linear<>
          (w3): Linear<>
          >
        >
      >
    (freqs_mgr): FreqsMgr<>
    (casual_mask): CausalMaskForQwen<>
    (kvcache_preprocess): KVCachePreprocess<>
    (ln_f): LlamaRMSNorm<>
    >
  (lm_head): Linear<>
  (loss): CrossEntropyLoss<
    (_softmax): _Softmax<>
    (_nllloss): _NLLLoss<>
    >
  >



```



```
import mindspore as ms
model = ms.load_checkpoint("/workspace/qwen-7b-chat-ms/qwen_7b_ms.ckpt")
for name, value in model.items():
    print(name)

transformer.wte.embedding_weight
transformer.layers.0.attention_norm.weight
transformer.layers.0.attention.wq.weight
transformer.layers.0.attention.wk.weight
transformer.layers.0.attention.wv.weight
transformer.layers.0.attention.wq.bias
transformer.layers.0.attention.wk.bias
transformer.layers.0.attention.wv.bias
transformer.layers.0.attention.wo.weight
transformer.layers.0.ffn_norm.weight
transformer.layers.0.feed_forward.w1.weight
transformer.layers.0.feed_forward.w3.weight
transformer.layers.0.feed_forward.w2.weight
...
transformer.layers.31.attention_norm.weight
transformer.layers.31.attention.wq.weight
transformer.layers.31.attention.wk.weight
transformer.layers.31.attention.wv.weight
transformer.layers.31.attention.wq.bias
transformer.layers.31.attention.wk.bias
transformer.layers.31.attention.wv.bias
transformer.layers.31.attention.wo.weight
transformer.layers.31.ffn_norm.weight
transformer.layers.31.feed_forward.w1.weight
transformer.layers.31.feed_forward.w3.weight
transformer.layers.31.feed_forward.w2.weight
transformer.ln_f.weight
lm_head.weight
```

## qwen1.5


```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/workspace/Qwen1.5-7B-Chat", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("/workspace/Qwen1.5-7B-Chat", device_map="cpu", torch_dtype=torch.float16, trust_remote_code=True)
print(model)

for name, param in model.named_parameters():
    print(name)

/home/guodong/Qwen-7B-Chat


Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(151936, 4096)
    (layers): ModuleList(
      (0-31): 32 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): Qwen2RotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm()
        (post_attention_layernorm): Qwen2RMSNorm()
      )
    )
    (norm): Qwen2RMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)
)

for name, param in model.named_parameters():
...     print(name)
...
model.embed_tokens.weight
model.layers.0.self_attn.q_proj.weight
model.layers.0.self_attn.q_proj.bias
model.layers.0.self_attn.k_proj.weight
model.layers.0.self_attn.k_proj.bias
model.layers.0.self_attn.v_proj.weight
model.layers.0.self_attn.v_proj.bias
model.layers.0.self_attn.o_proj.weight
model.layers.0.mlp.gate_proj.weight
model.layers.0.mlp.up_proj.weight
model.layers.0.mlp.down_proj.weight
model.layers.0.input_layernorm.weight
model.layers.0.post_attention_layernorm.weight
...
model.layers.31.self_attn.q_proj.weight
model.layers.31.self_attn.q_proj.bias
model.layers.31.self_attn.k_proj.weight
model.layers.31.self_attn.k_proj.bias
model.layers.31.self_attn.v_proj.weight
model.layers.31.self_attn.v_proj.bias
model.layers.31.self_attn.o_proj.weight
model.layers.31.mlp.gate_proj.weight
model.layers.31.mlp.up_proj.weight
model.layers.31.mlp.down_proj.weight
model.layers.31.input_layernorm.weight
model.layers.31.post_attention_layernorm.weight
model.norm.weight
lm_head.weight

```

```
import mindspore as ms
model = ms.load_checkpoint("/workspace/qwen1.5-7b-chat-ms/qwen_7b_ms.ckpt")
for name, value in model.items():
    print(name)


model.tok_embeddings.embedding_weight
model.layers.0.attention.wq.weight
model.layers.0.attention.wq.bias
model.layers.0.attention.wk.weight
model.layers.0.attention.wk.bias
model.layers.0.attention.wv.weight
model.layers.0.attention.wv.bias
model.layers.0.attention.wo.weight
model.layers.0.feed_forward.w1.weight
model.layers.0.feed_forward.w3.weight
model.layers.0.feed_forward.w2.weight
model.layers.0.attention_norm.weight
model.layers.0.ffn_norm.weight
...
model.layers.31.attention.wq.weight
model.layers.31.attention.wq.bias
model.layers.31.attention.wk.weight
model.layers.31.attention.wk.bias
model.layers.31.attention.wv.weight
model.layers.31.attention.wv.bias
model.layers.31.attention.wo.weight
model.layers.31.feed_forward.w1.weight
model.layers.31.feed_forward.w3.weight
model.layers.31.feed_forward.w2.weight
model.layers.31.attention_norm.weight
model.layers.31.ffn_norm.weight
model.norm_out.weight
lm_head.weight

```

