



```

docker run -it --name=mindie_server_t37 --net=host --ipc=host --privileged=true \
--device=/dev/davinci0 \
--device=/dev/davinci1 \
--device=/dev/davinci2 \
--device=/dev/davinci3 \
--device=/dev/davinci4 \
--device=/dev/davinci5 \
--device=/dev/davinci6 \
--device=/dev/davinci7 \
--device=/dev/davinci_manager \
--device=/dev/devmm_svm \
--device=/dev/hisi_hdc \
-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
-v /usr/local/Ascend/add-ons/:/usr/local/Ascend/add-ons/ \
-v /usr/local/sbin/:/usr/local/sbin/ \
-v /var/log/npu/slog/:/var/log/npu/slog \
-v /var/log/npu/profiling/:/var/log/npu/profiling \
-v /var/log/npu/dump/:/var/log/npu/dump \
-v /var/log/npu/:/usr/slog \
-v /etc/hccn.conf:/etc/hccn.conf \
-v /home:/workspace \
mindie_server:1.0.T37 \
/bin/bash


docker exec -it mindie_server_t37 bash



docker inspect mindie_server:1.0.T37

docker inspect -f '{{with .State}} {{.Pid}} {{end}}' mindie_server_t37


docker inspect --format='{{.Name}}' mindie_server_t37

docker inspect --format='ARCH: {{.Architecture}} , OS: {{.Os}}' mindie_server:1.0.T37
# ARCH: arm64 , OS: linux

docker inspect -f {{".Architecture"}} mindie_server:1.0.T37

```

{{range .NetworkSettings.Networks}}
```
source /usr/local/Ascend/ascend-toolkit/set_env.sh
source /usr/local/Ascend/mindie/set_env.sh
source /usr/local/Ascend/mindie/latest/mindie-service/set_env.sh
source /opt/atb-models/set_env.sh
```




```
cd /usr/local/Ascend/mindie/latest/mindie-service/latest
vim conf/config.json

```


npuMemSize: NPU中可以用来申请 kv cache的 size上限。单位：GB。
建议值：8。

npuMemSize=（总空闲-权重/tp数）*系数，其中系数取 0.8。

以 llama-65b为例：

总显存 64GB，空闲状态卡上有 3~4GB的占用，llama-65b
的总权重为 122GB，用 8张卡跑，则 npuMemSize取值的
上限为：(64-4-(122/8))*0.8。



```
/workspace/dataset/qwen
/workspace/aicc/model_from_hf/Baichuan2-7B-Chat
/workspace/aicc/model_from_hf/chatglm3-6b-chat
/workspace/aicc/model_from_hf/chatglm3-6b-chat-full
```



## Mindie_server

```
# 执行./mindieservice_daemon启动服务

cd /usr/local/Ascend/mindie/latest/mindie-service/latest/bin
./mindieservice_daemon

```


## 接口请求


```

curl -H "Accept: application/json" -H "Content-type: application/json" -X POST -d '{
"inputs": "保持健康的方法",
"parameters": {
"best_of": 1,
"decoder_input_details": true,
"details": true,
"do_sample": true,
"max_new_tokens": 20,
"repetition_penalty": 1.03,
"return_full_text": false,
"seed": null,
"stop": [
"photographer"
],
"top_k": 10,
"temperature": 0.5,
"top_n_tokens": 5,
"top_p": 0.95,
"truncate": null,
"typical_p": 0.95,
"watermark": true
},
"stream": false}' http://127.0.0.1:1025/generate



curl -H "Accept: application/json" -H "Content-type: application/json" -X POST -d '{
"inputs": "保持健康的方法",
"parameters": {
"best_of": 1,
"decoder_input_details": true,
"details": true,
"do_sample": true,
"max_new_tokens": 20,
"repetition_penalty": 1.03,
"return_full_text": false,
"seed": null,
"stop": [
"photographer"
],
"temperature": 0.5,
"top_n_tokens": 5,
"top_p": 0.95,
"truncate": null,
"typical_p": 0.95,
"watermark": true
},
"stream": false}' http://127.0.0.1:1025/generate




curl -H "Accept: application/json" -H "Content-type: application/json" -X POST -d '{
    "model": "qwen-72b",
    "messages": [
      {
        "role": "system",
        "content": "你是一个有博学多才的助手."
      },
      {
        "role": "user",
        "content": "保持健康的方法"
      }
    ]
  }' http://127.0.0.1:1025/v1/chat/completions




```




## 性能测试

```
cp token_input_gsm.csv
/usr/local/Ascend/mindie/latest/mindie-service/latest/bin
./llm_engine_test

```




```
cp token_input_gsm.csv /usr/local/Ascend/mindie/latest/mindie-service/latest/bin
cd /usr/local/Ascend/mindie/latest/mindie-service/latest/bin
./llm_engine_test

```



```


source /usr/local/Ascend/ascend-toolkit/set_env.sh
source /usr/local/Ascend/mindie/set_env.sh
source /usr/local/Ascend/mindie/latest/mindie-service/set_env.sh
source /opt/atb-models/set_env.sh


cd /opt/atb-models/tests/modeltest

bash run.sh pa_fp16 performance [[2048,2048]] 1 qwen /workspace/aicc/model_from_hf/qwen-72b-chat-hf 8 > qwen-72b-2048-1-8.log



bash run.sh pa_fp16 performance [[2048,2048],[2048,2048],[2048,2048],[2048,2048],[2048,2048]] 1 qwen /workspace/aicc/model_from_hf/qwen-72b-chat-hf 8 > qwen-72b-2048-1-8.log


bash run.sh pa_fp16 performance [[2048,2048]] 4 qwen /workspace/aicc/model_from_hf/qwen-72b-chat-hf 8 > qwen-72b-2048-4-8.log




```




```
2024-04-23 06:43:44,225 [INFO] [pid: 5844] generate.py-186: Prefill time: 299.3795871734619ms, Decode token time: 37.169588794344854ms, E2E time: 76385.52784919739ms


2024-04-23 06:43:45,396 - [INFO] - model_test.py:419 - batch: 1, seq_len_in: 2048, seq_len_out: 2048, total_time: 76.46122479438782, first_token_time: 299.38, non_first_token_time: 37.17, non_first_token_throughput: 26.90341673392521, e2e_time: 76.46122479438782, e2e_throughput: 26.784818128499577

2024-04-23 06:43:45,397 - [INFO] - model_test.py:434 - batch: 1, non_first_token_throughput_total: 26.90341673392521, non_first_token_throughput_average: 26.90341673392521, e2e_throughput_total: 26.784818128499577, e2e_throughput_average: 26.784818128499577

2024-04-23 06:43:45,399 - [INFO] - model_test.py:464 - qwen_72b  batch1 result saved in /opt/atb-models/tests/modeltest/base/../result/qwen_72b/pa_fp16_batch1_performance_test_result.csv
2024-04-23 06:43:45,399 - [INFO] - model_test.py:466 - qwen_72b  batch1 formatted result saved in /opt/atb-models/tests/modeltest/base/../result/qwen_72b/pa_fp16_batch1_performance_test_result_formatted.csv




2024-04-23 07:33:44,898 - [INFO] - model_test.py:419 - batch: 4, seq_len_in: 2048, seq_len_out: 2048, total_time: 86.89873886108398, first_token_time: 1157.03, non_first_token_time: 41.85, non_first_token_throughput: 95.57945041816009, e2e_time: 86.89873886108398, e2e_throughput: 94.27064313436932

2024-04-23 07:33:44,898 - [INFO] - model_test.py:434 - batch: 4, non_first_token_throughput_total: 95.57945041816009, non_first_token_throughput_average: 95.57945041816009, e2e_throughput_total: 94.27064313436932, e2e_throughput_average: 94.27064313436932
```







```python
def run_performance_test():
    non_first_token_throughput_total = 0
    e2e_throughput_total = 0
    for seq_len_in, seq_len_out in self.case_pair:
        self.logger.info("batch_size: " + str(self.batch_size) +
                         ", seq_len_in: " + str(seq_len_in) +
                         ", seq_len_out: " + str(seq_len_out))
        if self.model_type == "fa":
            input_ids = torch.randint(0, self.model.config.vocab_size, [self.batch_size, seq_len_in],
                                      dtype=torch.int64)
            attention_mask = torch.ones((self.batch_size, seq_len_in), dtype=torch.int64)
            inputs = self.tokenizer(performance_prompt * self.batch_size, return_tensors="pt",
                                    padding='max_length',
                                    max_length=seq_len_in)
            inputs["input_ids"] = input_ids
            inputs["attention_mask"] = attention_mask

            input_ids = inputs.input_ids.to(self.model.device)
            attention_mask = inputs.attention_mask.to(self.model.device)

            with torch.no_grad():
                getattr(torch, self.core_type).synchronize()
                e2e_start = time.time()
                generate_ids = self.model.generate(inputs=input_ids,
                                                   attention_mask=attention_mask,
                                                   min_new_tokens=seq_len_out,
                                                   max_new_tokens=seq_len_out
                                                   )
                try:
                    _ = self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True,
                                                    clean_up_tokenization_spaces=False)
                except:
                    _ = [
                        self.tokenizer.decode(output)
                        for output in generate_ids[:, inputs["input_ids"].size(1):].tolist()
                    ]
                getattr(torch, self.core_type).synchronize()
                e2e_end = time.time()
                e2e_time = e2e_end - e2e_start
        else:
            input_dict = {
                'rank': self.rank,
                'local_rank': self.local_rank,
                'world_size': self.world_size,
                'max_prefill_tokens': -1,
                'block_size': 128,
                'model_path': self.weight_dir,
                'is_bf16': True if self.data_type == "bf16" else False,
                'max_position_embeddings': self.max_position_embedding if self.max_position_embedding != -1 else seq_len_in + seq_len_out,
                'max_batch_size': self.batch_size,
                'use_refactor': self.use_refactor,
                'max_input_length': seq_len_in,
                'max_output_length': seq_len_out
            }
            pa_runner = PARunner(**input_dict)
            self.logger.info(str(self.rank) + f'pa_runner: {pa_runner}')
            pa_runner.warm_up()
            input_ids = torch.randint(0, pa_runner.model.config.vocab_size, [seq_len_in],
                                      dtype=torch.int64)
            _, _, e2e_time = pa_runner.infer("", self.batch_size, seq_len_out, True, [input_ids])
            del pa_runner
            torch.npu.empty_cache()

        if self.rank == 0:
            if self.model_type == "fa":
                first_token_time_tensor = torch.load(f"{folder_path}/first_token_time.pth").cpu()
                first_token_time = first_token_time_tensor.item()
                non_first_token_time_tensor = torch.load(f"{folder_path}/non_first_token_time.pth").cpu()
                non_first_token_time = non_first_token_time_tensor.item() / (seq_len_out - 1)
            else:
                benchmark_csv = os.path.join(self.script_path, "../benchmark.csv")
                with open(benchmark_csv, newline='') as csvfile:
                    csv_reader = csv.reader(csvfile)
                    next(csv_reader)
                    second_row = next(csv_reader)
                    first_token_time = float(second_row[4]) / 1000
                    non_first_token_time = float(second_row[5]) / 1000

            non_first_token_throughput = self.batch_size / non_first_token_time
            non_first_token_throughput_total += non_first_token_throughput
            e2e_throughput = self.batch_size * seq_len_out / e2e_time
            e2e_throughput_total += e2e_throughput

            self.logger.info(
                f"batch: {self.batch_size}, seq_len_in: {seq_len_in}, seq_len_out: {seq_len_out}, total_time: {e2e_time}, first_token_time: {first_token_time * 1000}," +
                f" non_first_token_time: {non_first_token_time * 1000}, non_first_token_throughput: {non_first_token_throughput}," +
                f" e2e_time: {e2e_time}, e2e_throughput: {e2e_throughput}")
            csv_results.append(
                [str(self.model_name).ljust(15), str(self.batch_size).ljust(15), str(seq_len_in).ljust(15),
                 str(seq_len_out).ljust(15),
                 str(round(e2e_time, 10)).ljust(15), str(round(first_token_time * 1000, 10)).ljust(25),
                 str(round(non_first_token_time * 1000, 10)).ljust(25),
                 str(round(non_first_token_throughput, 10)).ljust(36),
                 str(round(e2e_throughput, 10)).ljust(25)])

    if self.rank == 0:
        non_first_token_throughput_average = non_first_token_throughput_total / len(self.case_pair)
        e2e_throughput_average = e2e_throughput_total / len(self.case_pair)
        self.logger.info(
            f"batch: {self.batch_size}, non_first_token_throughput_total: {non_first_token_throughput_total}, non_first_token_throughput_average:" +
            f" {non_first_token_throughput_average}, e2e_throughput_total: {e2e_throughput_total}, e2e_throughput_average: {e2e_throughput_average}")
        csv_results[len(self.case_pair) - 1].extend(
            [str(round(non_first_token_throughput_average, 10)).ljust(45),
             str(round(e2e_throughput_average, 10)).ljust(35)])
        folder_name = self.model_name
        csv_name = self.model_type + "_" + self.data_type + "_" + self.test_mode + "_batch" + str(self.batch_size) + "_test_result.csv"
        if self.quantize:
            csv_name = self.model_type + "_" + self.data_type + "_" + self.quantize + "_batch" + str(self.batch_size) + "_" + self.test_mode + "_test_result.csv"
            csv_formatted_name = self.model_type + "_" + self.data_type + "_" + self.quantize + "_batch" + str(self.batch_size) + "_" + self.test_mode + "_test_result_formatted.csv"
        else:
            csv_name = self.model_type + "_" + self.data_type + "_batch" + str(self.batch_size) + "_" + self.test_mode + "_test_result.csv"
            csv_formatted_name = self.model_type + "_" + self.data_type + "_batch" + str(self.batch_size) + "_" + self.test_mode + "_test_result_formatted.csv"
        csv_performance_path = os.path.join(self.script_path, "../result", folder_name, csv_name)
        csv_performance_formatted_path = os.path.join(self.script_path, "../result", folder_name, csv_formatted_name)
        if not os.path.exists(csv_performance_formatted_path):
            self.logger.warning("performance result csv formatted file not exist, skip recording results")
            raise RuntimeError(f"csv result formatted file not exist")
        with open(csv_performance_formatted_path, 'a', newline='') as csv_file:
            csv_writer = csv.writer(csv_file, delimiter='|')
            for csv_result in csv_results:
                csv_writer.writerow(csv_result)

        csv_results.insert(0, ["Model", "Batchsize", "In_seq", "Out_seq", "Total time(s)", "First token time(ms)", "Non-first token time(ms)",
                              "Non-first token Throughout(Tokens/s)", "Throughout(Tokens/s)", "Non-first token Throughout Average(Tokens/s)",
                              "E2E Throughout Average(Tokens/s)"])
        df = pd.DataFrame(csv_results)
        df.to_csv(csv_performance_path, index=False, header=False)

        self.logger.info(self.model_name + " " + " batch" + str(
            self.batch_size) + " result saved in " + csv_performance_path)
        self.logger.info(self.model_name + " " + " batch" + str(
            self.batch_size) + " formatted result saved in " + csv_performance_formatted_path)

```


