

```
git lfs clone https://www.modelscope.cn/qwen/Qwen1.5-7B-Chat.git
git lfs clone https://huggingface.co/Qwen/Qwen1.5-7B-Chat

git lfs clone https://www.modelscope.cn/qwen/Qwen1.5-7B-Chat.git
git lfs clone https://www.modelscope.cn/baichuan-inc/Baichuan2-7B-Chat.git
git lfs clone https://www.modelscope.cn/baichuan-inc/Baichuan2-13B-Chat.git
git lfs clone https://www.modelscope.cn/qwen/Qwen1.5-14B-Chat.git
```

- https://ascendhub.huawei.com/#/detail/ascend-mindspore


```
docker login -u 157xxx4031 ascendhub.huawei.com
docker pull ascendhub.huawei.com/public-ascendhub/ascend-mindspore:23.0.0-A2-ubuntu18.04
```





```
-e ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \


docker rm -f pytorch_ubuntu_dev
docker run -it -u root \
--name pytorch_ubuntu_dev \
--network host \
-e ASCEND_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
-v /etc/localtime:/etc/localtime \
-v /var/log/npu/:/usr/slog \
-v /usr/bin/hccn_tool:/usr/bin/hccn_tool \
-v /data/containerd/workspace/:/workspace \
ascendhub.huawei.com/public-ascendhub/ascend-mindspore:23.0.0-A2-ubuntu18.04 \
/bin/bash

docker start pytorch_ubuntu_dev
docker exec -it pytorch_ubuntu_dev bash

```


https://repo.anaconda.com/miniconda/Miniconda3-py39_24.4.0-0-Linux-aarch64.sh\

wget -c https://repo.anaconda.com/miniconda/Miniconda3-py39_24.4.0-0-Linux-aarch64.sh

bash Miniconda3-py39_24.4.0-0-Linux-aarch64.sh -p /workspace/installs/conda

export PATH=/root/miniconda3/bin:$PATH

conda list



conda create -n llm-dev python=3.9
conda activate llm-dev 




```
docker exec -it pytorch_ubuntu_dev bash
conda activate llm-dev 
source /usr/local/Ascend/ascend-toolkit/set_env.sh
cd /workspace/code/stanford_alpaca
```


pip list | grep -E 'deepspeed|peft|torch|torch-npu|transformers|mindspore|mindformers|tqdm'



```
pip3 install torch==2.1.0
pip3 install pyyaml setuptools
pip3 install torch-npu==2.1.0
pip3 install numpy attrs decorator psutil absl-py cloudpickle psutil scipy synr tornado





source /usr/local/Ascend/ascend-toolkit/set_env.sh


import torch
import torch_npu

x = torch.randn(2, 2).npu()
y = torch.randn(2, 2).npu()
z = x.mm(y)

print(z)
```


/usr/local/Ascend
find . -name libascend_hal.so



```
# >>> conda initialize >>>
# !! Contents within this block are managed by 'conda init' !!
__conda_setup="$('/root/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
if [ $? -eq 0 ]; then
    eval "$__conda_setup"
else
    if [ -f "/root/miniconda3/etc/profile.d/conda.sh" ]; then
        . "/root/miniconda3/etc/profile.d/conda.sh"
    else
        export PATH="/workspace/installs/miniconda3/bin:$PATH"
    fi
fi
unset __conda_setup
# <<< conda initialize <<<
export PATH="/workspace/installs/miniconda3/bin:$PATH"
```

----



```
git clone https://github.com/tatsu-lab/stanford_alpaca.git



# pip uninstall Xformers

`ck_decoderF` is not supported because:
    device=npu (supported: {'cuda'})
    attn_bias type is <class 'xformers.ops.fmha.attn_bias.LowerTriangularMask'>
    operator wasn't built - see `python -m xformers.info` for more info
`ckF` is not supported because:
    device=npu (supported: {'cuda'})
    operator wasn't built - see `python -m xformers.info` for more info
    raise NotImplementedError(msg)

    



pip install accelerate -U
pip install -r requirements.txt
pip install deepspeed==0.14.1


删除utils.py中openai的代码。








--fsdp "full_shard auto_wrap" \
--fsdp_transformer_layer_cls_to_wrap 'Qwen2DecoderLayer'


alpaca_gpt4_data_zh_1k.json

rm -rf /workspace/output/alpaca2/*
torchrun --nproc_per_node=8 --master_port=29001 train.py \
    --model_name_or_path /workspace/model/Qwen1.5-7B-Chat/ \
    --data_path /workspace/data/alpaca_gpt4_data_zh_5k.json \
    --fp16 True \
    --output_dir /workspace/output/alpaca2 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps=8 \
    --num_train_epochs 1 \
    --per_device_eval_batch_size 4 \
    --eval_strategy "no" \
    --save_strategy "steps" \
    --save_steps 2000 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --deepspeed ds_config_zero3.json




source /usr/local/Ascend/ascend-toolkit/set_env.sh
conda activate llm-dev



alpaca_gpt4_data_zh_1k.json

torchrun --nproc_per_node=8 --master_port=29001 train.py \
    --model_name_or_path /workspace/model/Qwen1.5-14B-Chat/ \
    --data_path ./alpaca_data_1k.json \
    --fp16 True \
    --output_dir /workspace/output/alpaca-qwen14 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --gradient_checkpointing True \
    --eval_strategy "no" \
    --save_strategy "steps" \
    --save_steps 2000 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "full_shard offload auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap 'Qwen2DecoderLayer'

```




goodput 实际吞吐量





```
/workspace/output/baichuan2



torchrun --nproc_per_node=8 --master_port=29001 train.py \
    --model_name_or_path /workspace/model/Baichuan2-7B-Chat \
    --data_path ./alpaca_data_1k.json \
    --fp16 True \
    --output_dir /workspace/output/baichuan2 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --eval_strategy "no" \
    --save_strategy "steps" \
    --save_steps 2000 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "full_shard auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap 'DecoderLayer'
```



----


```

git clone https://gitee.com/ascend/apex.git



git clone https://gitee.com/ascend/ModelLink.git
git clone https://github.com/NVIDIA/Megatron-LM.git

``









