## Alpaca

仅支持单轮对话（alpaca_data.json）：

```
{
    "instruction": "Give three tips for staying healthy.",
    "input": "",
    "output": "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule."
}
```

- https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py#L127

```
dict(input_ids=self.input_ids[i], labels=self.labels[i])
```

## FastChat（Vicuna）

- 参考：https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py

单轮对话（alpaca-data-conversation.json）：

```
{
"id": "1",
"conversations": [
  {
    "from": "human",
    "value": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Response:"
  },
  {
    "from": "gpt",
    "value": "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule."
  }
]
}
```

多轮对话（dummy_conversation.json）：

```
{
"id": "identity_2",
"conversations": [
  {
    "from": "human",
    "value": "What is up?"
  },
  {
    "from": "gpt",
    "value": "Hello! How can I help you today?"
  },
  {
    "from": "human",
    "value": "Who are you?"
  },
  {
    "from": "gpt",
    "value": "You can call me Vicuna, and I was trained by Large Model Systems Organization (LMSYS) researchers as a language model."
  },
  {
    "from": "human",
    "value": "Goodbye"
  },
  {
    "from": "gpt",
    "value": "Goodbye! If you have any more questions in the future, don't hesitate to ask."
  }
]
}
```


## ChatGLM2

每条数据格式为: `[gMASK]sop [Round 1]\n\n问：{input1}\n\n答：{target1}</s>[Round 2]\n\n问：{input2}\n\n答：{target2}</s>...`


- https://github.com/THUDM/ChatGLM2-6B/blob/921d7e9adc69020a19169d1ba4f76c2675a2dd29/ptuning/main.py#L180

训练：

```
{"prompt": "长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线", "response": "用电脑能读数据流吗？水温多少", "history": []}
{"prompt": "95", "response": "上下水管温差怎么样啊？空气是不是都排干净了呢？", "history": [["长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线", "用电脑能读数据流吗？水温多少"]]}
{"prompt": "是的。上下水管都好的", "response": "那就要检查线路了，一般风扇继电器是由电脑控制吸合的，如果电路存在断路，或者电脑坏了的话会出现继电器不吸合的情况！", "history": [["长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线", "用电脑能读数据流吗？水温多少"], ["95", "上下水管温差怎么样啊？空气是不是都排干净了呢？"]]}

```

```python
def preprocess_function_train(examples):
    max_seq_length = data_args.max_source_length + data_args.max_target_length + 1

    model_inputs = {
        "input_ids": [],
        "labels": [],
    }
    for i in range(len(examples[prompt_column])):


        if examples[prompt_column][i] and examples[response_column][i]:

            query, answer = examples[prompt_column][i], examples[response_column][i]

            history = examples[history_column][i] if history_column is not None else None


            prompt = tokenizer.build_prompt(query, history)

            prompt = prefix + prompt
            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,
                                     max_length=data_args.max_source_length)


            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,
                                     max_length=data_args.max_target_length)

            context_length = len(a_ids)
            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]
            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]
            
            pad_len = max_seq_length - len(input_ids)
            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len
            labels = labels + [tokenizer.pad_token_id] * pad_len
            if data_args.ignore_pad_token_for_loss:
                labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]

            model_inputs["input_ids"].append(input_ids)
            model_inputs["labels"].append(labels)

    return model_inputs
```


- https://huggingface.co/THUDM/chatglm2-6b/blob/main/tokenization_chatglm.py#L162


```python
def get_prefix_tokens(self):
    prefix_tokens = [self.get_command("[gMASK]"), self.get_command("sop")]
    return prefix_tokens

# 推理
def build_prompt(self, query, history=None):
    if history is None:
        history = []
    prompt = ""
    for i, (old_query, response) in enumerate(history):
        prompt += "[Round {}]\n\n问：{}\n\n答：{}\n\n".format(i + 1, old_query, response)
    prompt += "[Round {}]\n\n问：{}\n\n答：".format(len(history) + 1, query)
    return prompt


def build_inputs_with_special_tokens(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
) -> List[int]:
    """
    Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
    adding special tokens. A BERT sequence has the following format:
    - single sequence: `[CLS] X [SEP]`
    - pair of sequences: `[CLS] A [SEP] B [SEP]`
    Args:
        token_ids_0 (`List[int]`):
            List of IDs to which the special tokens will be added.
        token_ids_1 (`List[int]`, *optional*):
            Optional second list of IDs for sequence pairs.
    Returns:
        `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
    """
    prefix_tokens = self.get_prefix_tokens()
    token_ids_0 = prefix_tokens + token_ids_0
    if token_ids_1 is not None:
        token_ids_0 = token_ids_0 + token_ids_1 + [self.get_command("<eos>")]
    return token_ids_0

```


- https://github.com/THUDM/ChatGLM2-6B/blob/main/cli_demo.py#L19

推理：
```
def build_prompt(history):
    prompt = "欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序"
    for query, response in history:
        prompt += f"\n\n用户：{query}"
        prompt += f"\n\nChatGLM2-6B：{response}"
    return prompt
```



## ChatGLM3

参考：

- https://github.com/THUDM/ChatGLM3/blob/main/PROMPT.md
- https://github.com/THUDM/ChatGLM3/blob/main/finetune_demo/finetune_hf.py
- https://github.com/tangqiaoyu/ToolAlpaca

单轮对话（参考Alpaca）：

```
{"context": "hello", "target": "hi,I am ChatGLM3"}
```

其中，context是对话的上文，也就是模型的输入，target是对话的下文，也就是模型的输出。

基座模型不支持对话，工具，代码生成等能力，仅支持文本生成。如果你需要对话能力，请使用Chat模型和对应的微调框架。

- https://github.com/THUDM/ChatGLM3/blob/main/finetune_chatmodel_demo/preprocess_utils.py#L112

InputOutputDataset

```
{
    "input_ids": input_ids,
    "labels": labels
}
```

多轮对话：

ChatGLM3 对话的格式由若干对话组成，其中每个对话包含对话头和内容，一个典型的多轮对话结构如下：

```
<|system|>
You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.
<|user|>
Hello
<|assistant|>
Hello, I'm ChatGLM3. What can I assist you today?
```

实际中每轮对话内容并不一定以换行符结尾，这里只是为了美观。

多轮（仅对话）：

```
[
  {
    "conversations": [
      {
        "role": "system",
        "content": "<system prompt text>"
      },
      {
        "role": "user",
        "content": "<user prompt text>"
      },
      {
        "role": "assistant",
        "content": "<assistant response text>"
      }, 
       // ... Muti Turn
      {
        "role": "user",
        "content": "<user prompt text>"
      },
      {
        "role": "assistant",
        "content": "<assistant response text>"
      }
    ]
  }
  // ...
]
```

多轮（对话+工具）：

```json
[
   {
      "tools": [
         // available tools, format is not restricted
      ],
      "conversations": [
         {
            "role": "system",
            "content": "<system prompt text>"
         },
         {
            "role": "user",
            "content": "<user prompt text>"
         },
         {
            "role": "assistant",
            "content": "<assistant thought to text>"
         },
         {
            "role": "tool",
            "name": "<name of the tool to be called",
            "parameters": {
               "<parameter_name>": "<parameter_value>"
            },
            "observation": "<observation>"
            // don't have to be string
         },
         {
            "role": "assistant",
            "content": "<assistant response to observation>"
         },
         // ... Muti Turn
         {
            "role": "user",
            "content": "<user prompt text>"
         },
         {
            "role": "assistant",
            "content": "<assistant response text>"
         }
      ]
   }
   // ...
]
```

- https://github.com/THUDM/ChatGLM3/blob/main/finetune_chatmodel_demo/preprocess_utils.py#L82

MultiTurnDataset

```
{
    "input_ids": tokens,
    "labels": labels
}
```

## Baichuan


- https://github.com/baichuan-inc/Baichuan-13B/issues/25
- https://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py
- https://huggingface.co/baichuan-inc/Baichuan-13B-Chat/blob/main/generation_utils.py#L7


```
user_token_id：195 -> <reserved_102>
assistant_token_id: 196 -> <reserved_103>

<reserved_102>{query}<reserved_103>{answer}



# 2. id拼接方式
[195] + tokenizer.encode("你好") + [196] + tokenizer.encode("我是人工智能助理")
# [195, 9875, 31213, 196, 6323, 31161, 22073, 1974
```



## Baichuan2

参考：

```
 {
  "id": "77771",
  "conversations": [
   {
    "from": "human",
    "value": "写一篇关于气候变化对海洋生态的影响的文章。\n"
   },
   {
    "from": "gpt",
    "value": "好的，以下是你要求的文章：\n气候变化对海洋生态的影响\n气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\n此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\n总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\n"
   },
   {
    "from": "human",
    "value": "从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\n"
   },
   {
    "from": "gpt",
    "value": "文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。"
   }
  ]
 }
```

- https://github.com/baichuan-inc/Baichuan2/blob/main/fine-tune/fine-tune.py#L70

```python

def preprocessing(self, example):
    input_ids = []
    labels = []

    for message in example["conversations"]:
        from_ = message["from"]

        value = message["value"]
        value_ids = self.tokenizer.encode(value)

        if from_ == "human":
            input_ids += self.user_tokens + value_ids
            labels += [self.tokenizer.eos_token_id] + [self.ignore_index] * len(
                value_ids
            )
        else:
            input_ids += self.assistant_tokens + value_ids
            labels += [self.ignore_index] + value_ids

    input_ids.append(self.tokenizer.eos_token_id)
    labels.append(self.tokenizer.eos_token_id)

    input_ids = input_ids[: self.model_max_length]
    labels = labels[: self.model_max_length]

    input_ids += [self.tokenizer.pad_token_id] * (
        self.model_max_length - len(input_ids)
    )

    labels += [self.ignore_index] * (self.model_max_length - len(labels))

    input_ids = torch.LongTensor(input_ids)
    labels = torch.LongTensor(labels)

    attention_mask = input_ids.ne(self.tokenizer.pad_token_id)

    return {
        "input_ids": input_ids,
        "labels": labels,
        "attention_mask": attention_mask,
    }
```


- https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat/blob/main/tokenization_baichuan.py#L174

```python
def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
    bos_token_id = [self.bos_token_id] if self.add_bos_token else []
    eos_token_id = [self.eos_token_id] if self.add_eos_token else []

    output = bos_token_id + token_ids_0 + eos_token_id

    if token_ids_1 is not None:
        output = output + bos_token_id + token_ids_1 + eos_token_id

    return output

```




## ModelScope-Agent

参考：

- https://github.com/modelscope/modelscope-agent
- https://modelscope.cn/datasets/damo/MSAgent-Bench/summary

MSAgent-Bench数据集主要包括了四种：AI模型API，通用API，API无关通用sft数据，API检索增强数据

```
{
    "id":"modelscope_merge_api_527",
    "conversations":[
        {
            "from":"system",
            "value":"你是达摩院的ModelScopeGPT（魔搭助手），你是个大语言模型， 是2023年达摩院的工程师训练得到的。你有多种能力，可以通过插件集成魔搭社区的模型api来回复用户的问题，还能解答用户使用模型遇到的问题和模型知识相关问答。1. {\"plugin_name\": \"modelscope_text-ie\", \"plugin_owner\": \"ModelScopeGPT\", \"plugin_type\": \"default\", \"plugin_schema_for_model\": {\"name\": \"modelscope_text-ie\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"url\": \"http://109.199.101.10:1485/\", \"paths\": [{\"name\": \"modelscope_text-ie\", \"model_id\": \"/damo/nlp_structbert_siamese-uie_chinese-base\", \"method\": \"post\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"parameters\": [{\"name\": \"text\", \"description\": \"用户输入的文本\", \"required\": \"True\"}, {\"name\": \"schema\", \"description\": \"要抽取信息的json表示\", \"required\": \"True\"}]}]}}\n\n2. {\"plugin_name\": \"modelscope_text-ie\", \"plugin_owner\": \"ModelScopeGPT\", \"plugin_type\": \"default\", \"plugin_schema_for_model\": {\"name\": \"modelscope_text-ie\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"url\": \"http://9.32.64.200:5873/\", \"paths\": [{\"name\": \"modelscope_text-ie\", \"model_id\": \"/damo/nlp_structbert_siamese-uie_chinese-base\", \"method\": \"post\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"parameters\": [{\"name\": \"text\", \"description\": \"用户输入的文本\", \"required\": \"True\"}, {\"name\": \"schema\", \"description\": \"要抽取信息的json表示\", \"required\": \"True\"}]}]}}\n\n3. {\"plugin_name\": \"modelscope_text-ie\", \"plugin_owner\": \"ModelScopeGPT\", \"plugin_type\": \"default\", \"plugin_schema_for_model\": {\"name\": \"modelscope_text-ie\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"url\": \"http://54.149.78.185:3979/\", \"paths\": [{\"name\": \"modelscope_text-ie\", \"model_id\": \"/damo/nlp_structbert_siamese-uie_chinese-base\", \"method\": \"post\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"parameters\": [{\"name\": \"text\", \"description\": \"用户输入的文本\", \"required\": \"True\"}, {\"name\": \"schema\", \"description\": \"要抽取信息的json表示\", \"required\": \"True\"}]}]}}"
        },
        {
            "from":"user",
            "value":"按照给定的schema抽取出下面文本对应的信息\nschema：{\"人物\": null, \"地理位置\": null, \"组织机构\": null}\n近日，美国政府宣布将对中国1000多种商品加征关税，并威胁进一步加征关税。"
        },
        {
            "from":"assistant",
            "value":"<|startofthink|>```JSON\n{\"api_name\": \"modelscope_text-ie\", \"url\": \"http://9.32.64.200:5873/damo/nlp_structbert_siamese-uie_chinese-base\", \"parameters\": {\"text\": \"近日，美国政府宣布将对中国1000多种商品加征关税，并威胁进一步加征关税。\", \"schema\": \"{\\\"人物\\\": null, \\\"地理位置\\\": null, \\\"组织机构\\\": null}\"}}\n```<|endofthink|>\n\n<|startofexec|>```JSON\n{\"人物\": [], \"地理位置\": [\"中国\", \"美国\"], \"组织机构\": []}\n```<|endofexec|>\n信息抽取结果：{\"人物\": [], \"地理位置\": [\"中国\", \"美国\"], \"组织机构\": []}。我使用的模型是ModelScope的'damo/nlp_structbert_siamese-uie_chinese-base'模型。这是一个基于StructBERT预训练模型微调训练的通用信息抽取模型。"
        }
    ]
}
```

数据格式说明：

id和converstions两个字段，其中conversations里面包含了system，user，assistant三种字段。其中：

- system: 表示给模型前置的人设输入，其中有告诉模型如何调用插件以及生成请求
- user: 表示用户的输入prompt，分为两种，通用生成的prompt和调用插件需求的prompt
- assistant: 为模型的回复。其中会包括插件调用代码和执行代码，调用代码是要LLM生成的，而执行代码是调用服务来生成结果的。
  - 比如：调用部分代码会通过<|startofthink|>和<|endofthink|>包起来，然后执行部分代码是api执行完结果后，把执行结果通过<|startofexec|>和<|endofexec|>包起来再输入给模型生成后面的回复

startofthink-endofthink：

```
{
	"api_name": "modelscope_text-ie",
	"url": "http://9.32.64.200:5873/damo/nlp_structbert_siamese-uie_chinese-base",
	"parameters": {
		"text": "近日，美国政府宣布将对中国1000多种商品加征关税，并威胁进一步加征关税。",
		"schema": "{\"人物\": null, \"地理位置\": null, \"组织机构\": null}"
	}
}
```

startofexec-endofexec：

```
{
	"人物": [],
	"地理位置": ["中国", "美国"],
	"组织机构": []
}
```

**推理链路搭建设计到LLM的推理和API的调用**

- 模型生成完整的<|startofthink|>和<|endofthink|>后，需要我们实时的去请求对应的API
- 返回结果后<|startofexec|>和<|endofexec|>拼接到现有的输入
- 然后再让大模型继续生成回复
- https://github.com/modelscope/modelscope-agent/blob/master/demo/tool_agent_finetune_swift/llm_sft.py
- https://github.com/modelscope/modelscope-agent/blob/master/demo/tool_agent_finetune_swift/utils/dataset.py#L18

```
def get_ms_tool_dataset(dataset_name_or_file) -> HfDataset:

	dataset = HfDataset.from_dict({
	    'inputs': all_inputs_str,
	    'flags': all_inputs_flag
	})
	return dataset
```


## Openai-Chatml

## Firefly

- https://github.com/yangjianxin1/Firefly/blob/master/component/template.py#L172
- https://github.com/yangjianxin1/Firefly/blob/master/component/dataset.py


## QWen



## qwen1.5

- 官网示例：https://qwen.readthedocs.io/en/latest/training/SFT/example.html

```
{
    "type": "chatml",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "Tell me something about large language models."
        },
        {
            "role": "assistant",
            "content": "Large language models are a type of language model that is trained on a large corpus of text data. They are capable of generating human-like text and are used in a variety of natural language processing tasks..."
        }
    ],
    "source": "unknown"
}

{
    "type": "chatml",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is your name?"
        },
        {
            "role": "assistant",
            "content": "My name is Qwen."
        }
    ],
    "source": "self-made"
}
```

- qwen2：https://github.com/huggingface/transformers/blob/ff841900e45763114d2417fb24ce29d950c6c956/docs/source/en/model_doc/qwen2.md


训练：

- https://github.com/QwenLM/Qwen1.5/blob/main/examples/sft/finetune.py#L146
```
messages = [example["messages"] for example in raw_data]

for i, msg in enumerate(messages):
        texts.append(
            tokenizer.apply_chat_template(
                msg,
                tokenize=True,
                add_generation_prompt=False,
                padding=True,
                max_length=max_len,
                truncation=True,
            )
        )
```


推理：

- https://github.com/QwenLM/Qwen1.5/blob/main/examples/demo/cli_demo.py#L126

```
conversation = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
]
for query_h, response_h in history:
    conversation.append({'role': 'user', 'content': query_h})
    conversation.append({'role': 'assistant', 'content': response_h})

conversation.append({'role': 'user', 'content': query})
inputs = tokenizer.apply_chat_template(
    conversation,
    add_generation_prompt=True,
    return_tensors='pt',
)
```




## QWen-Agent



## InternLM



## hf-transformers


- https://github.com/huggingface/transformers/blob/ff841900e45763114d2417fb24ce29d950c6c956/src/transformers/tokenization_utils_base.py#L1847


```
def apply_chat_template(
        self,
        conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]], "Conversation"],
        chat_template: Optional[str] = None,
        add_generation_prompt: bool = False,
        tokenize: bool = True,
        padding: bool = False,
        truncation: bool = False,
        max_length: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_dict: bool = False,
        tokenizer_kwargs: Optional[Dict[str, Any]] = None,
        **kwargs,
    )
```

- 官网示例：https://huggingface.co/docs/transformers/main/chat_templating


默认对话模板（default chat template）：

```
"{% for message in messages %}"
"{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}"
"{% endfor %}"
"{% if add_generation_prompt %}"
"{{ '<|im_start|>assistant\n' }}"
"{% endif %}"
```



```
<|im_start|>system
You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>
<|im_start|>user
How are you?<|im_end|>
<|im_start|>assistant
I'm doing great!<|im_end|>
```




### 模型单独定义默认的模板 


示例：blenderbot 模型


- https://github.com/huggingface/transformers/blob/ff841900e45763114d2417fb24ce29d950c6c956/src/transformers/models/blenderbot/tokenization_blenderbot.py#L89


- loop.last:如果是最后一次迭代，为True
- loop.first:如果是第一次迭代，为True

```
(
    "{% for message in messages %}"
    "{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}"
    "{{ message['content'] }}"
    "{% if not loop.last %}{{ '  ' }}{% endif %}"
    "{% endfor %}"
    "{{ eos_token }}"
)
```


```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

chat = [
   {"role": "user", "content": "Hello, how are you?"},
   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
   {"role": "user", "content": "I'd like to show off how chat templating works!"},
]

tokenizer.apply_chat_template(chat, tokenize=False)
```

```
" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>"

```



### LLaMA

```
{% for message in messages %}
    {% if message['role'] == 'user' %}
        {{ bos_token + '[INST] ' + message['content'] + ' [/INST]' }}
    {% elif message['role'] == 'system' %}
        {{ '<<SYS>>\\n' + message['content'] + '\\n<</SYS>>\\n\\n' }}
    {% elif message['role'] == 'assistant' %}
        {{ ' '  + message['content'] + ' ' + eos_token }}
    {% endif %}
{% endfor %}

```



