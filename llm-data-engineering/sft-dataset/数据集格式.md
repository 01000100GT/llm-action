## Alpaca

仅支持单轮对话（alpaca_data.json）：

```
{
    "instruction": "Give three tips for staying healthy.",
    "input": "",
    "output": "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule."
}
```

- https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py#L127

```
dict(input_ids=self.input_ids[i], labels=self.labels[i])
```

## FastChat（Vicuna）

- 参考：https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py

单轮对话（alpaca-data-conversation.json）：

```
{
"id": "1",
"conversations": [
  {
    "from": "human",
    "value": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Response:"
  },
  {
    "from": "gpt",
    "value": "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule."
  }
]
}
```

多轮对话（dummy_conversation.json）：

```
{
"id": "identity_2",
"conversations": [
  {
    "from": "human",
    "value": "What is up?"
  },
  {
    "from": "gpt",
    "value": "Hello! How can I help you today?"
  },
  {
    "from": "human",
    "value": "Who are you?"
  },
  {
    "from": "gpt",
    "value": "You can call me Vicuna, and I was trained by Large Model Systems Organization (LMSYS) researchers as a language model."
  },
  {
    "from": "human",
    "value": "Goodbye"
  },
  {
    "from": "gpt",
    "value": "Goodbye! If you have any more questions in the future, don't hesitate to ask."
  }
]
}
```

## ChatGLM3

参考：

- https://github.com/THUDM/ChatGLM3/blob/main/PROMPT.md
- https://github.com/THUDM/ChatGLM3/tree/main/finetune_chatmodel_demo
- https://github.com/tangqiaoyu/ToolAlpaca

单轮对话（参考Alpaca）：

```
{"context": "hello", "target": "hi,I am ChatGLM3"}
```

其中，context是对话的上文，也就是模型的输入，target是对话的下文，也就是模型的输出。

基座模型不支持对话，工具，代码生成等能力，仅支持文本生成。如果你需要对话能力，请使用Chat模型和对应的微调框架。

- https://github.com/THUDM/ChatGLM3/blob/main/finetune_chatmodel_demo/preprocess_utils.py#L112

InputOutputDataset

```
{
    "input_ids": input_ids,
    "labels": labels
}
```

多轮对话：

ChatGLM3 对话的格式由若干对话组成，其中每个对话包含对话头和内容，一个典型的多轮对话结构如下：

```
<|system|>
You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.
<|user|>
Hello
<|assistant|>
Hello, I'm ChatGLM3. What can I assist you today?
```

实际中每轮对话内容并不一定以换行符结尾，这里只是为了美观。

多轮（仅对话）：

```
[
  {
    "conversations": [
      {
        "role": "system",
        "content": "<system prompt text>"
      },
      {
        "role": "user",
        "content": "<user prompt text>"
      },
      {
        "role": "assistant",
        "content": "<assistant response text>"
      }, 
       // ... Muti Turn
      {
        "role": "user",
        "content": "<user prompt text>"
      },
      {
        "role": "assistant",
        "content": "<assistant response text>"
      }
    ]
  }
  // ...
]
```

多轮（对话+工具）：

```json
[
   {
      "tools": [
         // available tools, format is not restricted
      ],
      "conversations": [
         {
            "role": "system",
            "content": "<system prompt text>"
         },
         {
            "role": "user",
            "content": "<user prompt text>"
         },
         {
            "role": "assistant",
            "content": "<assistant thought to text>"
         },
         {
            "role": "tool",
            "name": "<name of the tool to be called",
            "parameters": {
               "<parameter_name>": "<parameter_value>"
            },
            "observation": "<observation>"
            // don't have to be string
         },
         {
            "role": "assistant",
            "content": "<assistant response to observation>"
         },
         // ... Muti Turn
         {
            "role": "user",
            "content": "<user prompt text>"
         },
         {
            "role": "assistant",
            "content": "<assistant response text>"
         }
      ]
   }
   // ...
]
```

- https://github.com/THUDM/ChatGLM3/blob/main/finetune_chatmodel_demo/preprocess_utils.py#L82

MultiTurnDataset

```
{
    "input_ids": tokens,
    "labels": labels
}
```

## Baichuan2

参考：

```
 {
  "id": "77771",
  "conversations": [
   {
    "from": "human",
    "value": "写一篇关于气候变化对海洋生态的影响的文章。\n"
   },
   {
    "from": "gpt",
    "value": "好的，以下是你要求的文章：\n气候变化对海洋生态的影响\n气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\n此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\n总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\n"
   },
   {
    "from": "human",
    "value": "从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\n"
   },
   {
    "from": "gpt",
    "value": "文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。"
   }
  ]
 }
```

- https://github.com/baichuan-inc/Baichuan2/blob/main/fine-tune/fine-tune.py#L70

```
def preprocessing(self, example):
	return {
        "input_ids": input_ids,
        "labels": labels,
        "attention_mask": attention_mask,
    }
```

## ModelScope-Agent

参考：

- https://github.com/modelscope/modelscope-agent
- https://modelscope.cn/datasets/damo/MSAgent-Bench/summary

MSAgent-Bench数据集主要包括了四种：AI模型API，通用API，API无关通用sft数据，API检索增强数据

```
{
    "id":"modelscope_merge_api_527",
    "conversations":[
        {
            "from":"system",
            "value":"你是达摩院的ModelScopeGPT（魔搭助手），你是个大语言模型， 是2023年达摩院的工程师训练得到的。你有多种能力，可以通过插件集成魔搭社区的模型api来回复用户的问题，还能解答用户使用模型遇到的问题和模型知识相关问答。1. {\"plugin_name\": \"modelscope_text-ie\", \"plugin_owner\": \"ModelScopeGPT\", \"plugin_type\": \"default\", \"plugin_schema_for_model\": {\"name\": \"modelscope_text-ie\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"url\": \"http://109.199.101.10:1485/\", \"paths\": [{\"name\": \"modelscope_text-ie\", \"model_id\": \"/damo/nlp_structbert_siamese-uie_chinese-base\", \"method\": \"post\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"parameters\": [{\"name\": \"text\", \"description\": \"用户输入的文本\", \"required\": \"True\"}, {\"name\": \"schema\", \"description\": \"要抽取信息的json表示\", \"required\": \"True\"}]}]}}\n\n2. {\"plugin_name\": \"modelscope_text-ie\", \"plugin_owner\": \"ModelScopeGPT\", \"plugin_type\": \"default\", \"plugin_schema_for_model\": {\"name\": \"modelscope_text-ie\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"url\": \"http://9.32.64.200:5873/\", \"paths\": [{\"name\": \"modelscope_text-ie\", \"model_id\": \"/damo/nlp_structbert_siamese-uie_chinese-base\", \"method\": \"post\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"parameters\": [{\"name\": \"text\", \"description\": \"用户输入的文本\", \"required\": \"True\"}, {\"name\": \"schema\", \"description\": \"要抽取信息的json表示\", \"required\": \"True\"}]}]}}\n\n3. {\"plugin_name\": \"modelscope_text-ie\", \"plugin_owner\": \"ModelScopeGPT\", \"plugin_type\": \"default\", \"plugin_schema_for_model\": {\"name\": \"modelscope_text-ie\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"url\": \"http://54.149.78.185:3979/\", \"paths\": [{\"name\": \"modelscope_text-ie\", \"model_id\": \"/damo/nlp_structbert_siamese-uie_chinese-base\", \"method\": \"post\", \"description\": \"针对中文的文本，根据schema要抽取的内容，找出其中对应信息，并用json格式展示\", \"parameters\": [{\"name\": \"text\", \"description\": \"用户输入的文本\", \"required\": \"True\"}, {\"name\": \"schema\", \"description\": \"要抽取信息的json表示\", \"required\": \"True\"}]}]}}"
        },
        {
            "from":"user",
            "value":"按照给定的schema抽取出下面文本对应的信息\nschema：{\"人物\": null, \"地理位置\": null, \"组织机构\": null}\n近日，美国政府宣布将对中国1000多种商品加征关税，并威胁进一步加征关税。"
        },
        {
            "from":"assistant",
            "value":"<|startofthink|>```JSON\n{\"api_name\": \"modelscope_text-ie\", \"url\": \"http://9.32.64.200:5873/damo/nlp_structbert_siamese-uie_chinese-base\", \"parameters\": {\"text\": \"近日，美国政府宣布将对中国1000多种商品加征关税，并威胁进一步加征关税。\", \"schema\": \"{\\\"人物\\\": null, \\\"地理位置\\\": null, \\\"组织机构\\\": null}\"}}\n```<|endofthink|>\n\n<|startofexec|>```JSON\n{\"人物\": [], \"地理位置\": [\"中国\", \"美国\"], \"组织机构\": []}\n```<|endofexec|>\n信息抽取结果：{\"人物\": [], \"地理位置\": [\"中国\", \"美国\"], \"组织机构\": []}。我使用的模型是ModelScope的'damo/nlp_structbert_siamese-uie_chinese-base'模型。这是一个基于StructBERT预训练模型微调训练的通用信息抽取模型。"
        }
    ]
}
```

数据格式说明：

id和converstions两个字段，其中conversations里面包含了system，user，assistant三种字段。其中：

- system: 表示给模型前置的人设输入，其中有告诉模型如何调用插件以及生成请求
- user: 表示用户的输入prompt，分为两种，通用生成的prompt和调用插件需求的prompt
- assistant: 为模型的回复。其中会包括插件调用代码和执行代码，调用代码是要LLM生成的，而执行代码是调用服务来生成结果的。
  - 比如：调用部分代码会通过<|startofthink|>和<|endofthink|>包起来，然后执行部分代码是api执行完结果后，把执行结果通过<|startofexec|>和<|endofexec|>包起来再输入给模型生成后面的回复

startofthink-endofthink：

```
{
	"api_name": "modelscope_text-ie",
	"url": "http://9.32.64.200:5873/damo/nlp_structbert_siamese-uie_chinese-base",
	"parameters": {
		"text": "近日，美国政府宣布将对中国1000多种商品加征关税，并威胁进一步加征关税。",
		"schema": "{\"人物\": null, \"地理位置\": null, \"组织机构\": null}"
	}
}
```

startofexec-endofexec：

```
{
	"人物": [],
	"地理位置": ["中国", "美国"],
	"组织机构": []
}
```

**推理链路搭建设计到LLM的推理和API的调用**

- 模型生成完整的<|startofthink|>和<|endofthink|>后，需要我们实时的去请求对应的API
- 返回结果后<|startofexec|>和<|endofexec|>拼接到现有的输入
- 然后再让大模型继续生成回复
- https://github.com/modelscope/modelscope-agent/blob/master/demo/tool_agent_finetune_swift/llm_sft.py
- https://github.com/modelscope/modelscope-agent/blob/master/demo/tool_agent_finetune_swift/utils/dataset.py#L18

```
def get_ms_tool_dataset(dataset_name_or_file) -> HfDataset:

	dataset = HfDataset.from_dict({
	    'inputs': all_inputs_str,
	    'flags': all_inputs_flag
	})
	return dataset
```


## Openai-Chatml



## QWen



## qwen1.5

- 官网示例：https://qwen.readthedocs.io/en/latest/training/SFT/example.html

```
{
    "type": "chatml",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "Tell me something about large language models."
        },
        {
            "role": "assistant",
            "content": "Large language models are a type of language model that is trained on a large corpus of text data. They are capable of generating human-like text and are used in a variety of natural language processing tasks..."
        }
    ],
    "source": "unknown"
}

{
    "type": "chatml",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is your name?"
        },
        {
            "role": "assistant",
            "content": "My name is Qwen."
        }
    ],
    "source": "self-made"
}
```

- qwen2：https://github.com/huggingface/transformers/blob/ff841900e45763114d2417fb24ce29d950c6c956/docs/source/en/model_doc/qwen2.md


训练：

- https://github.com/QwenLM/Qwen1.5/blob/main/examples/sft/finetune.py#L146
```
messages = [example["messages"] for example in raw_data]

for i, msg in enumerate(messages):
        texts.append(
            tokenizer.apply_chat_template(
                msg,
                tokenize=True,
                add_generation_prompt=False,
                padding=True,
                max_length=max_len,
                truncation=True,
            )
        )
```


推理：

- https://github.com/QwenLM/Qwen1.5/blob/main/examples/demo/cli_demo.py#L126

```
conversation = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
]
for query_h, response_h in history:
    conversation.append({'role': 'user', 'content': query_h})
    conversation.append({'role': 'assistant', 'content': response_h})

conversation.append({'role': 'user', 'content': query})
inputs = tokenizer.apply_chat_template(
    conversation,
    add_generation_prompt=True,
    return_tensors='pt',
)
```




## QWen-Agent



## InternLM



## hf-transformers


- https://github.com/huggingface/transformers/blob/ff841900e45763114d2417fb24ce29d950c6c956/src/transformers/tokenization_utils_base.py#L1847


```
def apply_chat_template(
        self,
        conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]], "Conversation"],
        chat_template: Optional[str] = None,
        add_generation_prompt: bool = False,
        tokenize: bool = True,
        padding: bool = False,
        truncation: bool = False,
        max_length: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_dict: bool = False,
        tokenizer_kwargs: Optional[Dict[str, Any]] = None,
        **kwargs,
    )
```

- 官网示例：https://huggingface.co/docs/transformers/main/chat_templating


默认对话模板（default chat template）：

```
"{% for message in messages %}"
"{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}"
"{% endfor %}"
"{% if add_generation_prompt %}"
"{{ '<|im_start|>assistant\n' }}"
"{% endif %}"
```



```
<|im_start|>system
You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>
<|im_start|>user
How are you?<|im_end|>
<|im_start|>assistant
I'm doing great!<|im_end|>
```




### 模型单独定义默认的模板 


示例：blenderbot 模型


- https://github.com/huggingface/transformers/blob/ff841900e45763114d2417fb24ce29d950c6c956/src/transformers/models/blenderbot/tokenization_blenderbot.py#L89


- loop.last:如果是最后一次迭代，为True
- loop.first:如果是第一次迭代，为True

```
(
    "{% for message in messages %}"
    "{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}"
    "{{ message['content'] }}"
    "{% if not loop.last %}{{ '  ' }}{% endif %}"
    "{% endfor %}"
    "{{ eos_token }}"
)
```


```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

chat = [
   {"role": "user", "content": "Hello, how are you?"},
   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
   {"role": "user", "content": "I'd like to show off how chat templating works!"},
]

tokenizer.apply_chat_template(chat, tokenize=False)
```

```
" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>"

```



### LLaMA

```
{% for message in messages %}
    {% if message['role'] == 'user' %}
        {{ bos_token + '[INST] ' + message['content'] + ' [/INST]' }}
    {% elif message['role'] == 'system' %}
        {{ '<<SYS>>\\n' + message['content'] + '\\n<</SYS>>\\n\\n' }}
    {% elif message['role'] == 'assistant' %}
        {{ ' '  + message['content'] + ' ' + eos_token }}
    {% endif %}
{% endfor %}

```



